"""
æ€§åˆ«å’ŒèŒä¸šè¯çš„embeddingåˆ†æ

åŠŸèƒ½ï¼š
1. æŒ‰çœä»½åˆ†ç»„æ•°æ®è®­ç»ƒWord2Vecæ¨¡å‹
2. è®¡ç®—èŒä¸šè¯ä¸æ€§åˆ«è¯çš„å…³è”åº¦ï¼ˆåˆ†åˆ«è®¡ç®—ä¸ç”·æ€§è¯ã€å¥³æ€§è¯çš„ç›¸ä¼¼åº¦ï¼‰
3. æ¯”è¾ƒä¸åŒçœä»½æ¨¡å‹çš„å·®å¼‚

è¾“å…¥æ•°æ®ï¼šcleaned_weibo_cov/{year}/ ä¸‹çš„parquetæ–‡ä»¶
"""

import os
import pandas as pd
import numpy as np
from gensim.models import Word2Vec
from collections import defaultdict
import jieba
import fire
from sklearn.preprocessing import normalize
import warnings
import json

warnings.filterwarnings("ignore")

DATA_DIR = "cleaned_weibo_cov"
OUTPUT_DIR = "embedding_analysis"

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
os.makedirs(OUTPUT_DIR, exist_ok=True)

# æ€§åˆ«è¯è¡¨ï¼ˆæ‰©å±•ç‰ˆï¼‰
GENDER_WORDS = {
    "male": [
        # ä»£è¯
        "ä»–",
        "ä»–ä»¬",
        "ä»–çš„",
        # åŸºç¡€æ€§åˆ«è¯
        "ç”·",
        "ç”·äºº",
        "ç”·æ€§",
        "ç”·å­",
        "ç”·ç”Ÿ",
        "ç”·å­©",
        # ç§°è°“
        "å…ˆç”Ÿ",
        "å¸…å“¥",
        "å°ä¼™",
        "å°ä¼™å­",
        "å“¥",
        "å…„å¼Ÿ",
        "çˆ·ä»¬",
        # å®¶åº­è§’è‰²
        "çˆ¶äº²",
        "çˆ¸çˆ¸",
        "çˆ¸",
        "å„¿å­",
        "ä¸ˆå¤«",
        "è€å…¬",
        "ç”·å‹",
        "ç”·æœ‹å‹",
    ],
    "female": [
        # ä»£è¯
        "å¥¹",
        "å¥¹ä»¬",
        "å¥¹çš„",
        # åŸºç¡€æ€§åˆ«è¯
        "å¥³",
        "å¥³äºº",
        "å¥³æ€§",
        "å¥³å­",
        "å¥³ç”Ÿ",
        "å¥³å­©",
        # ç§°è°“
        "å¥³å£«",
        "å°å§",
        "ç¾å¥³",
        "å§‘å¨˜",
        "å°å§‘å¨˜",
        "å§",
        "å¦¹",
        "å§å¦¹",
        "é—ºèœœ",
        # å®¶åº­è§’è‰²
        "æ¯äº²",
        "å¦ˆå¦ˆ",
        "å¦ˆ",
        "å¥³å„¿",
        "é—ºå¥³",
        "å¦»å­",
        "è€å©†",
        "å¥³å‹",
        "å¥³æœ‹å‹",
    ],
}

# èŒä¸šè¯è¡¨ï¼ˆæ‰©å±•ç‰ˆï¼ŒæŒ‰é¢„æœŸæ€§åˆ«åˆ»æ¿ç¨‹åº¦åˆ†ç±»ï¼‰
OCCUPATION_WORDS = {
    # é¢„æœŸåå¥³æ€§çš„èŒä¸š
    "female_stereotyped": [
        "æŠ¤å£«",
        "å¹¼å¸ˆ",
        "å¹¼å„¿æ•™å¸ˆ",
        "ä¿å§†",
        "æœˆå«‚",
        "ç§˜ä¹¦",
        "å‰å°",
        "æ–‡å‘˜",
        "å®¢æœ",
        "æ”¶é“¶å‘˜",
        "å¯¼è´­",
        "ç¾å®¹å¸ˆ",
        "åŒ–å¦†å¸ˆ",
        "ç©ºå§",
        "æ¨¡ç‰¹",
        "ç‘œä¼½æ•™ç»ƒ",
    ],
    # é¢„æœŸåç”·æ€§çš„èŒä¸š
    "male_stereotyped": [
        "ç¨‹åºå‘˜",
        "å·¥ç¨‹å¸ˆ",
        "å¸æœº",
        "å¨å¸ˆ",
        "ä¿å®‰",
        "å»ºç­‘å·¥",
        "å¿«é€’å‘˜",
        "å¤–å–å‘˜",
        "ç”µå·¥",
        "æœºæ¢°å¸ˆ",
        "å†›äºº",
        "è­¦å¯Ÿ",
        "æ¶ˆé˜²å‘˜",
        "é£è¡Œå‘˜",
        "èˆ¹å‘˜",
    ],
    # é¢„æœŸç›¸å¯¹ä¸­æ€§çš„èŒä¸š
    "neutral": [
        "æ•™å¸ˆ",
        "è€å¸ˆ",
        "åŒ»ç”Ÿ",
        "ä¼šè®¡",
        "å¾‹å¸ˆ",
        "è®°è€…",
        "è®¾è®¡å¸ˆ",
        "ç¿»è¯‘",
        "ä½œå®¶",
        "æ¼”å‘˜",
        "æ­Œæ‰‹",
        "ç»ç†",
        "é”€å”®",
        "å…¬åŠ¡å‘˜",
        "èŒå‘˜",
    ],
    # é«˜åœ°ä½èŒä¸š
    "high_status": [
        "è€æ¿",
        "æ€»è£",
        "è‘£äº‹é•¿",
        "CEO",
        "é™¢é•¿",
        "æ ¡é•¿",
        "æ•™æˆ",
        "ç§‘å­¦å®¶",
        "ç ”ç©¶å‘˜",
        "ä¸“å®¶",
        "åšå£«",
    ],
}

# åˆå¹¶æ‰€æœ‰èŒä¸šè¯
ALL_OCCUPATIONS = []
for category in OCCUPATION_WORDS.values():
    ALL_OCCUPATIONS.extend(category)

# é€šç”¨åœç”¨è¯
STOPWORDS = set(
    [
        "çš„",
        "æ˜¯",
        "äº†",
        "åœ¨",
        "æœ‰",
        "å’Œ",
        "å°±",
        "ä¸",
        "äºº",
        "éƒ½",
        "ä¸€",
        "ä¸€ä¸ª",
        "ä¸Š",
        "ä¹Ÿ",
        "å¾ˆ",
        "åˆ°",
        "è¯´",
        "è¦",
        "å»",
        "ä½ ",
        "ä¼š",
        "ç€",
        "æ²¡æœ‰",
        "çœ‹",
        "å¥½",
        "è‡ªå·±",
        "è¿™",
        "é‚£",
        "æˆ‘",
        "ä»–",
        "å¥¹",
        "æˆ‘ä»¬",
        "ä½ ä»¬",
        "ä»–ä»¬",
        "å¥¹ä»¬",
        "ä»€ä¹ˆ",
        "æ€ä¹ˆ",
        "è¿™ä¸ª",
        "é‚£ä¸ª",
    ]
)


def preprocess_text(text):
    """é¢„å¤„ç†æ–‡æœ¬ï¼Œåˆ†è¯å¹¶è¿‡æ»¤åœç”¨è¯"""
    if pd.isna(text) or text == "":
        return []

    text = str(text)
    words = jieba.cut(text)
    words = [
        w.strip()
        for w in words
        if w.strip() and w not in STOPWORDS and len(w.strip()) > 1
    ]
    return words


def load_data_by_province(year):
    """æŒ‰çœä»½åŠ è½½æ•°æ®ï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
    year_dir = os.path.join(DATA_DIR, str(year))
    if not os.path.exists(year_dir):
        print(f"âŒ æœªæ‰¾åˆ° {year} å¹´çš„æ•°æ®ç›®å½•")
        return None

    import glob

    pattern = os.path.join(year_dir, "*.parquet")
    parquet_files = sorted(glob.glob(pattern))

    if not parquet_files:
        print(f"âŒ æœªæ‰¾åˆ° {year} å¹´çš„æ•°æ®æ–‡ä»¶")
        return None

    print(f"ğŸ“‚ æ‰¾åˆ° {len(parquet_files)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹åŠ è½½...")

    # åªåŠ è½½éœ€è¦çš„åˆ—ï¼Œå‡å°‘å†…å­˜å ç”¨
    required_columns = ["weibo_content"]
    province_col = None

    # å…ˆæ£€æŸ¥ç¬¬ä¸€ä¸ªæ–‡ä»¶ç¡®å®šçœä»½å­—æ®µåï¼ˆåªè¯»å–ä¸€è¡Œï¼Œå‡å°‘å†…å­˜å ç”¨ï¼‰
    province_col = "province"
    required_columns.append("province")

    if province_col is None:
        print(f"âŒ æ— æ³•ç¡®å®šçœä»½å­—æ®µ")
        return None

    data_by_province = defaultdict(list)

    for file_idx, file_path in enumerate(parquet_files):
        try:
            # åªè¯»å–éœ€è¦çš„åˆ—
            df = pd.read_parquet(file_path, columns=required_columns)

            # è¿‡æ»¤æ‰ç©ºå€¼
            df = df.dropna(subset=[province_col, "weibo_content"])

            # æŒ‰çœä»½åˆ†ç»„ï¼Œä½¿ç”¨å­—å…¸ç›´æ¥èšåˆè€Œä¸æ˜¯append
            for province in df[province_col].unique():
                province_data = df[df[province_col] == province].copy()
                data_by_province[province].append(province_data)

            # åŠæ—¶é‡Šæ”¾å†…å­˜
            del df

            if (file_idx + 1) % 10 == 0:
                print(f"  å·²å¤„ç† {file_idx + 1}/{len(parquet_files)} ä¸ªæ–‡ä»¶...")

        except Exception as e:
            print(f"âŒ è¯»å–æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
            continue

    # åˆå¹¶æ¯ä¸ªçœä»½çš„æ•°æ®ï¼ˆä½¿ç”¨concatä½†åŠæ—¶é‡Šæ”¾ï¼‰
    print(f"\nğŸ“Š æŒ‰çœä»½åˆ†ç»„ï¼Œå…± {len(data_by_province)} ä¸ªçœä»½")

    result = {}
    for province, data_list in data_by_province.items():
        # åˆå¹¶æ•°æ®
        combined_data = pd.concat(data_list, ignore_index=True)
        # ç«‹å³é‡Šæ”¾åŸåˆ—è¡¨å†…å­˜
        del data_list

        if len(combined_data) > 1000:  # è‡³å°‘1000æ¡æ•°æ®
            result[province] = combined_data
            print(f"  âœ“ {province}: {len(combined_data):,} æ¡æ•°æ®")
        else:
            print(f"  âœ— {province}: {len(combined_data):,} æ¡æ•°æ® (è·³è¿‡ï¼Œæ•°æ®é‡ä¸è¶³)")
            del combined_data

    return result


def train_word2vec(texts, vector_size=300, window=5, min_count=20, workers=None):
    """
    è®­ç»ƒWord2Vecæ¨¡å‹ï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ï¼‰

    å‚æ•°è°ƒæ•´è¯´æ˜ï¼š
    - vector_size: 300ï¼ˆæ›´å¤§çš„å‘é‡ç»´åº¦ï¼Œæ›´å¥½çš„è¯­ä¹‰è¡¨è¾¾ï¼‰
    - window: 5ï¼ˆä¸Šä¸‹æ–‡çª—å£ï¼‰
    - min_count: 20ï¼ˆè¯é¢‘é˜ˆå€¼ï¼Œæ ¹æ®æ•°æ®é‡è°ƒæ•´ï¼‰
    - workers: çº¿ç¨‹æ•°ï¼ŒNoneåˆ™è‡ªåŠ¨è®¾ç½®ä¸ºCPUæ ¸å¿ƒæ•°-1
    """
    if not texts or len(texts) < 100:
        return None

    # è‡ªåŠ¨è®¾ç½®workersï¼Œé¿å…è¶…è¿‡CPUæ ¸å¿ƒæ•°
    if workers is None:
        import multiprocessing

        workers = max(1, multiprocessing.cpu_count() - 1)

    # é™åˆ¶workersæ•°é‡ï¼Œé¿å…å†…å­˜è¿‡åº¦å ç”¨
    workers = min(workers, 8)

    model = Word2Vec(
        sentences=texts,
        vector_size=vector_size,
        window=window,
        min_count=min_count,
        workers=workers,  # å¤šçº¿ç¨‹ï¼Œä½†é™åˆ¶æ•°é‡
        epochs=10,
        sg=1,  # Skip-gramï¼ˆå¯¹ä¸­å°è§„æ¨¡æ•°æ®æ›´å¥½ï¼‰
        negative=10,  # è´Ÿé‡‡æ ·
        seed=42,  # å¯é‡å¤æ€§
        max_vocab_size=None,  # ä¸é™åˆ¶è¯æ±‡è¡¨å¤§å°ï¼Œä½†å¯ä»¥é€šè¿‡min_countæ§åˆ¶
    )

    return model


def get_word_embedding(model, word):
    """è·å–è¯å‘é‡"""
    try:
        return model.wv[word]
    except KeyError:
        return None


def get_word_set_embedding(model, words):
    """è·å–ä¸€ç»„è¯çš„å¹³å‡å‘é‡ï¼ˆå½’ä¸€åŒ–ï¼‰"""
    vectors = []
    found_words = []

    for word in words:
        vec = get_word_embedding(model, word)
        if vec is not None:
            vectors.append(vec)
            found_words.append(word)

    if not vectors:
        return None, []

    # è®¡ç®—å¹³å‡å‘é‡å¹¶å½’ä¸€åŒ–
    avg_vec = np.mean(vectors, axis=0)
    normalized_vec = normalize([avg_vec])[0]

    return normalized_vec, found_words


def cosine_similarity(vec1, vec2):
    """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))


def compute_gender_bias(occupation_vec, male_vec, female_vec):
    """
    è®¡ç®—èŒä¸šçš„æ€§åˆ«åå‘åˆ†æ•°

    è¿”å›ï¼š
        bias_score: æ­£å€¼=åå¥³æ€§ï¼Œè´Ÿå€¼=åç”·æ€§ï¼Œæ¥è¿‘0=ä¸­æ€§
        male_sim: ä¸ç”·æ€§è¯çš„ç›¸ä¼¼åº¦
        female_sim: ä¸å¥³æ€§è¯çš„ç›¸ä¼¼åº¦
    """
    male_sim = cosine_similarity(occupation_vec, male_vec)
    female_sim = cosine_similarity(occupation_vec, female_vec)

    # æ€§åˆ«åå‘åˆ†æ•° = å¥³æ€§ç›¸ä¼¼åº¦ - ç”·æ€§ç›¸ä¼¼åº¦
    bias_score = female_sim - male_sim

    return bias_score, male_sim, female_sim


def analyze_province_embedding(data_by_province, year):
    """åˆ†ææ¯ä¸ªçœä»½çš„embedding"""
    results = []
    province_stats = []

    for province, data in data_by_province.items():
        print(f"\n{'='*60}")
        print(f"ğŸ” å¤„ç†çœä»½: {province}")
        print(f"{'='*60}")

        # é¢„å¤„ç†æ–‡æœ¬ï¼ˆå†…å­˜ä¼˜åŒ–ï¼šä½¿ç”¨itertuplesè€Œä¸æ˜¯iterrowsï¼Œåˆ†æ‰¹å¤„ç†ï¼‰
        # å…ˆä¿å­˜æ•°æ®æ¡æ•°ï¼Œå› ä¸ºåé¢ä¼šåˆ é™¤DataFrame
        data_count = len(data)

        texts = []
        # ä½¿ç”¨itertuplesæ¯”iterrowså¿«å¾—å¤šä¸”å†…å­˜å ç”¨æ›´å°‘
        for row in data.itertuples():
            words = preprocess_text(row.weibo_content)
            if len(words) > 3:  # è‡³å°‘3ä¸ªè¯
                texts.append(words)

        # å¤„ç†å®Œæ–‡æœ¬åç«‹å³é‡Šæ”¾DataFrameå†…å­˜
        del data

        if len(texts) < 100:
            print(f"  âŒ æ–‡æœ¬é‡ä¸è¶³ ({len(texts)} æ¡)ï¼Œè·³è¿‡")
            del texts
            continue

        print(f"  ğŸ“ æœ‰æ•ˆæ–‡æœ¬: {len(texts):,} æ¡")

        # è®­ç»ƒæ¨¡å‹
        print(f"  ğŸ”§ è®­ç»ƒWord2Vecæ¨¡å‹...")
        model = train_word2vec(texts)
        if model is None:
            print(f"  âŒ è®­ç»ƒæ¨¡å‹å¤±è´¥")
            continue

        vocab_size = len(model.wv)
        print(f"  âœ“ æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œè¯æ±‡è¡¨å¤§å°: {vocab_size:,}")

        # è®­ç»ƒå®Œæˆåç«‹å³é‡Šæ”¾textsåˆ—è¡¨ï¼ˆå¯èƒ½å ç”¨å¤§é‡å†…å­˜ï¼‰
        text_count = len(texts)
        del texts
        import gc

        gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶

        # è®¡ç®—æ€§åˆ«è¯å‘é‡
        male_vec, male_found = get_word_set_embedding(model, GENDER_WORDS["male"])
        female_vec, female_found = get_word_set_embedding(model, GENDER_WORDS["female"])

        if male_vec is None or female_vec is None:
            print(f"  âŒ æ€§åˆ«è¯å‘é‡è®¡ç®—å¤±è´¥")
            continue

        print(f"  âœ“ æ‰¾åˆ°ç”·æ€§è¯: {len(male_found)}/{len(GENDER_WORDS['male'])} ä¸ª")
        print(
            f"    {', '.join(male_found[:10])}{'...' if len(male_found) > 10 else ''}"
        )
        print(f"  âœ“ æ‰¾åˆ°å¥³æ€§è¯: {len(female_found)}/{len(GENDER_WORDS['female'])} ä¸ª")
        print(
            f"    {', '.join(female_found[:10])}{'...' if len(female_found) > 10 else ''}"
        )

        # è®¡ç®—æ¯ä¸ªèŒä¸šè¯çš„æ€§åˆ«åå‘
        occupation_results = []
        found_occupations = []

        for occupation in ALL_OCCUPATIONS:
            occ_vec = get_word_embedding(model, occupation)
            if occ_vec is not None:
                bias_score, male_sim, female_sim = compute_gender_bias(
                    occ_vec, male_vec, female_vec
                )

                occupation_results.append(
                    {
                        "occupation": occupation,
                        "bias_score": float(bias_score),
                        "male_similarity": float(male_sim),
                        "female_similarity": float(female_sim),
                    }
                )
                found_occupations.append(occupation)

        if not occupation_results:
            print(f"  âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•èŒä¸šè¯")
            continue

        print(f"  âœ“ æ‰¾åˆ°èŒä¸šè¯: {len(found_occupations)}/{len(ALL_OCCUPATIONS)} ä¸ª")

        # æ’åºå¹¶å±•ç¤ºç»“æœ
        occupation_results_sorted = sorted(
            occupation_results, key=lambda x: x["bias_score"], reverse=True
        )

        print(f"\n  ğŸ“Š èŒä¸šæ€§åˆ«åå‘åˆ†æ:")
        print(f"\n  ğŸ”µ æœ€åå¥³æ€§çš„èŒä¸š (Top 5):")
        for i, occ in enumerate(occupation_results_sorted[:5], 1):
            print(
                f"    {i}. {occ['occupation']:8s} | åå‘åˆ†æ•°: {occ['bias_score']:+.3f} "
                f"| å¥³æ€§ç›¸ä¼¼åº¦: {occ['female_similarity']:.3f} "
                f"| ç”·æ€§ç›¸ä¼¼åº¦: {occ['male_similarity']:.3f}"
            )

        print(f"\n  ğŸ”´ æœ€åç”·æ€§çš„èŒä¸š (Top 5):")
        for i, occ in enumerate(occupation_results_sorted[-5:][::-1], 1):
            print(
                f"    {i}. {occ['occupation']:8s} | åå‘åˆ†æ•°: {occ['bias_score']:+.3f} "
                f"| å¥³æ€§ç›¸ä¼¼åº¦: {occ['female_similarity']:.3f} "
                f"| ç”·æ€§ç›¸ä¼¼åº¦: {occ['male_similarity']:.3f}"
            )

        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        bias_scores = [r["bias_score"] for r in occupation_results]
        stats = {
            "province": province,
            "data_count": data_count,
            "text_count": text_count,
            "vocab_size": vocab_size,
            "occupations_found": len(found_occupations),
            "male_words_found": len(male_found),
            "female_words_found": len(female_found),
            "mean_bias": float(np.mean(bias_scores)),
            "std_bias": float(np.std(bias_scores)),
            "min_bias": float(np.min(bias_scores)),
            "max_bias": float(np.max(bias_scores)),
            "range_bias": float(np.max(bias_scores) - np.min(bias_scores)),
        }
        province_stats.append(stats)

        print(f"\n  ğŸ“ˆ ç»Ÿè®¡æŒ‡æ ‡:")
        print(f"    å¹³å‡åå‘: {stats['mean_bias']:+.3f}")
        print(f"    æ ‡å‡†å·®ï¼ˆéš”ç¦»ç¨‹åº¦ï¼‰: {stats['std_bias']:.3f}")
        print(f"    åå‘èŒƒå›´: [{stats['min_bias']:+.3f}, {stats['max_bias']:+.3f}]")

        # ä¿å­˜è¯¦ç»†ç»“æœï¼ˆå…ˆè½¬æ¢å‘é‡ä¸ºåˆ—è¡¨ï¼Œé¿å…åç»­å†…å­˜å ç”¨ï¼‰
        result = {
            "province": province,
            "stats": stats,
            "male_vec": male_vec.tolist(),  # è½¬æ¢ä¸ºåˆ—è¡¨åï¼ŒåŸå§‹numpyæ•°ç»„å¯ä»¥é‡Šæ”¾
            "female_vec": female_vec.tolist(),
            "male_words_found": male_found,
            "female_words_found": female_found,
            "occupations_found": found_occupations,
            "occupation_results": occupation_results,
        }
        results.append(result)

        # ä¿å­˜ç»“æœåç«‹å³é‡Šæ”¾å‘é‡ï¼ˆå·²ç»è½¬æ¢ä¸ºåˆ—è¡¨ï¼ŒåŸå§‹numpyæ•°ç»„ä¸å†éœ€è¦ï¼‰
        del male_vec
        del female_vec

        # ä¿å­˜æ¨¡å‹
        model_path = os.path.join(OUTPUT_DIR, f"model_{year}_{province}.model")
        model.save(model_path)
        print(f"  ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")

        # ä¿å­˜æ¨¡å‹åé‡Šæ”¾æ¨¡å‹ï¼ˆé‡Šæ”¾å†…å­˜ï¼‰
        del model
        gc.collect()  # å†æ¬¡åƒåœ¾å›æ”¶

    return results, province_stats


def save_results(results, province_stats, year):
    """ä¿å­˜åˆ†æç»“æœ"""
    if not results:
        print("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•ç»“æœ")
        return

    print(f"\n{'='*60}")
    print(f"ğŸ’¾ ä¿å­˜ç»“æœ...")
    print(f"{'='*60}")

    # 1. ä¿å­˜çœä»½ç»Ÿè®¡ä¿¡æ¯
    stats_df = pd.DataFrame(province_stats)
    stats_file = os.path.join(OUTPUT_DIR, f"province_stats_{year}.csv")
    stats_df.to_csv(stats_file, index=False, encoding="utf-8-sig")
    print(f"âœ“ çœä»½ç»Ÿè®¡ä¿¡æ¯: {stats_file}")

    # 2. ä¿å­˜èŒä¸šæ€§åˆ«åå‘è¯¦ç»†æ•°æ®ï¼ˆé•¿æ ¼å¼ï¼‰
    occupation_data = []
    for result in results:
        province = result["province"]
        for occ in result["occupation_results"]:
            occupation_data.append(
                {
                    "province": province,
                    "occupation": occ["occupation"],
                    "bias_score": occ["bias_score"],
                    "male_similarity": occ["male_similarity"],
                    "female_similarity": occ["female_similarity"],
                }
            )

    occupation_df = pd.DataFrame(occupation_data)
    occupation_file = os.path.join(OUTPUT_DIR, f"occupation_bias_{year}.csv")
    occupation_df.to_csv(occupation_file, index=False, encoding="utf-8-sig")
    print(f"âœ“ èŒä¸šæ€§åˆ«åå‘æ•°æ®: {occupation_file}")

    # 3. ä¿å­˜å®½æ ¼å¼æ•°æ®ï¼ˆçœä»½Ã—èŒä¸šçŸ©é˜µï¼‰
    pivot_df = occupation_df.pivot_table(
        values="bias_score", index="occupation", columns="province", aggfunc="mean"
    )
    pivot_file = os.path.join(OUTPUT_DIR, f"occupation_bias_pivot_{year}.csv")
    pivot_df.to_csv(pivot_file, encoding="utf-8-sig")
    print(f"âœ“ èŒä¸šÃ—çœä»½çŸ©é˜µ: {pivot_file}")

    # 4. ä¿å­˜è¯¦ç»†å‘é‡æ•°æ®ï¼ˆJSONæ ¼å¼ï¼Œä¾¿äºåç»­åˆ†æï¼‰
    detailed_data = []
    for result in results:
        detailed_data.append(
            {
                "province": result["province"],
                "stats": result["stats"],
                "male_vec": result["male_vec"],
                "female_vec": result["female_vec"],
                "male_words_found": result["male_words_found"],
                "female_words_found": result["female_words_found"],
                "occupations_found": result["occupations_found"],
            }
        )

    detailed_file = os.path.join(OUTPUT_DIR, f"detailed_vectors_{year}.json")
    with open(detailed_file, "w", encoding="utf-8") as f:
        json.dump(detailed_data, f, ensure_ascii=False, indent=2)
    print(f"âœ“ è¯¦ç»†å‘é‡æ•°æ®: {detailed_file}")

    # 5. ç”Ÿæˆç®€è¦åˆ†ææŠ¥å‘Š
    report_file = os.path.join(OUTPUT_DIR, f"analysis_report_{year}.txt")
    with open(report_file, "w", encoding="utf-8") as f:
        f.write(f"{'='*60}\n")
        f.write(f"æ€§åˆ«-èŒä¸šEmbeddingåˆ†ææŠ¥å‘Š ({year}å¹´)\n")
        f.write(f"{'='*60}\n\n")

        f.write(f"åˆ†æçœä»½æ•°: {len(results)}\n")
        f.write(f"åˆ†æèŒä¸šæ•°: {len(ALL_OCCUPATIONS)}\n\n")

        f.write(f"{'='*60}\n")
        f.write(f"å„çœä»½æ€§åˆ«éš”ç¦»æŒ‡æ•°æ’åï¼ˆæ ‡å‡†å·®ï¼‰:\n")
        f.write(f"{'='*60}\n")
        stats_sorted = sorted(province_stats, key=lambda x: x["std_bias"], reverse=True)
        for i, stat in enumerate(stats_sorted, 1):
            f.write(
                f"{i:2d}. {stat['province']:10s} | "
                f"éš”ç¦»æŒ‡æ•°: {stat['std_bias']:.3f} | "
                f"å¹³å‡åå‘: {stat['mean_bias']:+.3f}\n"
            )

        f.write(f"\n{'='*60}\n")
        f.write(f"èŒä¸šæ€§åˆ«åå‘ä¸€è‡´æ€§åˆ†æ:\n")
        f.write(f"{'='*60}\n")

        # è®¡ç®—æ¯ä¸ªèŒä¸šåœ¨å„çœä»½çš„å¹³å‡åå‘
        occupation_avg = (
            occupation_df.groupby("occupation")["bias_score"]
            .agg(["mean", "std"])
            .sort_values("mean", ascending=False)
        )

        f.write(f"\næœ€åå¥³æ€§çš„èŒä¸šï¼ˆè·¨çœä»½å¹³å‡ï¼‰:\n")
        for i, (occ, row) in enumerate(occupation_avg.head(10).iterrows(), 1):
            f.write(
                f"  {i:2d}. {occ:15s} | å¹³å‡: {row['mean']:+.3f} | æ ‡å‡†å·®: {row['std']:.3f}\n"
            )

        f.write(f"\næœ€åç”·æ€§çš„èŒä¸šï¼ˆè·¨çœä»½å¹³å‡ï¼‰:\n")
        for i, (occ, row) in enumerate(
            occupation_avg.tail(10).iloc[::-1].iterrows(), 1
        ):
            f.write(
                f"  {i:2d}. {occ:15s} | å¹³å‡: {row['mean']:+.3f} | æ ‡å‡†å·®: {row['std']:.3f}\n"
            )

        f.write(f"\nèŒä¸šåå‘å·®å¼‚æœ€å¤§çš„ï¼ˆè·¨çœä»½æ ‡å‡†å·®æœ€å¤§ï¼‰:\n")
        occupation_var = occupation_avg.sort_values("std", ascending=False)
        for i, (occ, row) in enumerate(occupation_var.head(10).iterrows(), 1):
            f.write(
                f"  {i:2d}. {occ:15s} | å¹³å‡: {row['mean']:+.3f} | æ ‡å‡†å·®: {row['std']:.3f}\n"
            )

    print(f"âœ“ åˆ†ææŠ¥å‘Š: {report_file}")

    print(f"\nâœ… æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ° {OUTPUT_DIR}/ ç›®å½•")


def main(year: int, province: str = None):
    """
    è¿è¡Œembeddingåˆ†æ

    Args:
        year: å¹´ä»½
        province: æŒ‡å®šçœä»½ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™å¤„ç†æ‰€æœ‰çœä»½
    """
    print(f"\n{'='*60}")
    print(f"ğŸš€ å¼€å§‹åˆ†æ {year} å¹´æ•°æ®çš„æ€§åˆ«-èŒä¸šEmbedding")
    print(f"{'='*60}\n")

    # åŠ è½½æ•°æ®
    data_by_province = load_data_by_province(year)
    if not data_by_province:
        print("âŒ æ— æ³•åŠ è½½æ•°æ®")
        return

    # å¦‚æœæŒ‡å®šäº†çœä»½ï¼Œåªå¤„ç†è¯¥çœä»½
    if province:
        if province not in data_by_province:
            print(f"âŒ æœªæ‰¾åˆ°çœä»½: {province}")
            print(f"å¯ç”¨çœä»½: {', '.join(data_by_province.keys())}")
            return
        data_by_province = {province: data_by_province[province]}
        print(f"ğŸ¯ åªå¤„ç†çœä»½: {province}\n")

    # åˆ†æembedding
    results, province_stats = analyze_province_embedding(data_by_province, year)

    # ä¿å­˜ç»“æœ
    save_results(results, province_stats, year)

    print(f"\n{'='*60}")
    print(f"ğŸ‰ {year} å¹´embeddingåˆ†æå®Œæˆï¼")
    print(f"{'='*60}\n")


if __name__ == "__main__":
    fire.Fire(main)
