"""
æ€§åˆ«å’ŒèŒä¸šè¯çš„embeddingåˆ†æå™¨

åŠŸèƒ½ï¼š
1. åŠ è½½å·²è®­ç»ƒçš„Word2Vecæ¨¡å‹
2. è®¡ç®—èŒä¸šè¯ä¸æ€§åˆ«è¯çš„å…³è”åº¦ï¼ˆåˆ†åˆ«è®¡ç®—ä¸ç”·æ€§è¯ã€å¥³æ€§è¯çš„ç›¸ä¼¼åº¦ï¼‰
3. æ¯”è¾ƒä¸åŒçœä»½æ¨¡å‹çš„å·®å¼‚
4. åˆ†æå®¶åŠ¡åˆ†å·¥è¯æ±‡ï¼ˆå®¶åº­åœºåŸŸ vs å·¥ä½œåœºåŸŸï¼‰çš„æ€§åˆ«å·®å¼‚
5. ç”Ÿæˆåˆ†ææŠ¥å‘Šå’Œå¯è§†åŒ–æ•°æ®

è¾“å…¥ï¼šgender_embedding/embedding_models/{year}/ ä¸‹çš„æ¨¡å‹æ–‡ä»¶
è¾“å‡ºï¼šembedding_analysis/{year}/ ä¸‹çš„åˆ†æç»“æœ
"""

import os
import pandas as pd
import numpy as np
from gensim.models import KeyedVectors
import fire
from sklearn.preprocessing import normalize
import warnings
import json
import glob

warnings.filterwarnings("ignore")

MODEL_DIR = "gender_norms/gender_embedding/embedding_models"
OUTPUT_DIR = "gender_norms/gender_embedding/results/embedding_analysis"
WORDLISTS_DIR = "wordlists"

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(WORDLISTS_DIR, exist_ok=True)


def load_json_wordlist(filename):
    """
    ä»JSONæ–‡ä»¶åŠ è½½è¯è¡¨

    Args:
        filename: JSONè¯è¡¨æ–‡ä»¶åï¼ˆåœ¨wordlistsç›®å½•ä¸‹ï¼‰

    Returns:
        dict: è¯è¡¨å­—å…¸
    """
    filepath = os.path.join(WORDLISTS_DIR, filename)
    if not os.path.exists(filepath):
        print(f"âš ï¸  è¯è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
        return {}

    try:
        with open(filepath, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"âŒ åŠ è½½è¯è¡¨æ–‡ä»¶å¤±è´¥ {filepath}: {e}")
        return {}


def load_gender_words():
    """åŠ è½½æ€§åˆ«è¯è¡¨"""
    data = load_json_wordlist("gender_words.json")
    return {"male": data.get("male", []), "female": data.get("female", [])}


def load_occupation_words():
    """åŠ è½½èŒä¸šè¯è¡¨ï¼Œè¿”å›æ‰€æœ‰èŒä¸šè¯çš„åˆ—è¡¨"""
    data = load_json_wordlist("occupation_words.json")
    all_occupations = []
    for category in data.values():
        all_occupations.extend(category)
    return all_occupations


def load_domestic_work_words():
    """åŠ è½½å®¶åŠ¡åˆ†å·¥è¯è¡¨"""
    data = load_json_wordlist("domestic_work_words.json")
    return {"family": data.get("family", []), "work": data.get("work", [])}


# åŠ è½½è¯è¡¨
GENDER_WORDS = load_gender_words()
ALL_OCCUPATIONS = load_occupation_words()
DOMESTIC_WORK_WORDS = load_domestic_work_words()


def get_word_embedding(model, word):
    """è·å–è¯å‘é‡"""
    try:
        return model[word]
    except KeyError:
        return None


def get_word_set_embedding(model, words):
    """è·å–ä¸€ç»„è¯çš„å¹³å‡å‘é‡ï¼ˆå½’ä¸€åŒ–ï¼‰"""
    vectors = []
    found_words = []

    for word in words:
        vec = get_word_embedding(model, word)
        if vec is not None:
            vectors.append(vec)
            found_words.append(word)

    if not vectors:
        return None, []

    # è®¡ç®—å¹³å‡å‘é‡å¹¶å½’ä¸€åŒ–
    avg_vec = np.mean(vectors, axis=0)
    normalized_vec = normalize([avg_vec])[0]

    return normalized_vec, found_words


def cosine_similarity(vec1, vec2):
    """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))


def compute_gender_bias(occupation_vec, male_vec, female_vec):
    """
    è®¡ç®—èŒä¸šçš„æ€§åˆ«åå‘åˆ†æ•°ï¼ˆåŸºäºä½™å¼¦ç›¸ä¼¼åº¦å·®å€¼ï¼‰

    è¿”å›ï¼š
        bias_score: æ­£å€¼=åå¥³æ€§ï¼Œè´Ÿå€¼=åç”·æ€§ï¼Œæ¥è¿‘0=ä¸­æ€§
        male_sim: ä¸ç”·æ€§è¯çš„ç›¸ä¼¼åº¦
        female_sim: ä¸å¥³æ€§è¯çš„ç›¸ä¼¼åº¦
    """
    male_sim = cosine_similarity(occupation_vec, male_vec)
    female_sim = cosine_similarity(occupation_vec, female_vec)

    # æ€§åˆ«åå‘åˆ†æ•° = å¥³æ€§ç›¸ä¼¼åº¦ - ç”·æ€§ç›¸ä¼¼åº¦
    bias_score = female_sim - male_sim

    return bias_score, male_sim, female_sim


def compute_gender_bias_projection(occupation_vec, male_vec, female_vec):
    """
    è®¡ç®—èŒä¸šçš„æ€§åˆ«åå‘åˆ†æ•°ï¼ˆåŸºäºæ€§åˆ«è½´æŠ•å½±ï¼‰

    æ„å»ºæ€§åˆ«è½´ï¼šä»ç”·æ€§å‘é‡æŒ‡å‘å¥³æ€§å‘é‡çš„æ–¹å‘å‘é‡ï¼ˆæ­£å‘ä¸ºå¥³æ€§ï¼Œè´Ÿå‘ä¸ºç”·æ€§ï¼‰
    è®¡ç®—èŒä¸šè¯å‘é‡åœ¨æ€§åˆ«è½´ä¸Šçš„æŠ•å½±å€¼

    è¿”å›ï¼š
        projection_score: æŠ•å½±å€¼ï¼Œæ­£å€¼=åå¥³æ€§ï¼Œè´Ÿå€¼=åç”·æ€§ï¼Œæ¥è¿‘0=ä¸­æ€§
        gender_axis: æ€§åˆ«è½´æ–¹å‘å‘é‡ï¼ˆå½’ä¸€åŒ–ï¼‰
    """
    # æ„å»ºæ€§åˆ«è½´ï¼šå¥³æ€§å‘é‡ - ç”·æ€§å‘é‡ï¼ˆæ­£å‘ä¸ºå¥³æ€§æ–¹å‘ï¼‰
    gender_axis = female_vec - male_vec

    # å½’ä¸€åŒ–æ€§åˆ«è½´
    axis_norm = np.linalg.norm(gender_axis)
    if axis_norm > 0:
        gender_axis_normalized = gender_axis / axis_norm
    else:
        # å¦‚æœæ€§åˆ«è½´ä¸ºé›¶å‘é‡ï¼Œè¿”å›0
        return 0.0, gender_axis

    # è®¡ç®—èŒä¸šè¯å‘é‡åœ¨æ€§åˆ«è½´ä¸Šçš„æŠ•å½±
    # projection = dot(occupation_vec, gender_axis_normalized)
    projection_score = np.dot(occupation_vec, gender_axis_normalized)

    return projection_score, gender_axis_normalized


def compute_domain_bias(word_vec, family_vec, work_vec):
    """
    è®¡ç®—è¯æ±‡åœ¨å®¶åº­åœºåŸŸ vs å·¥ä½œåœºåŸŸçš„åå‘åˆ†æ•°ï¼ˆåŸºäºä½™å¼¦ç›¸ä¼¼åº¦å·®å€¼ï¼‰

    è¿”å›ï¼š
        bias_score: æ­£å€¼=åå®¶åº­åœºåŸŸï¼Œè´Ÿå€¼=åå·¥ä½œåœºåŸŸï¼Œæ¥è¿‘0=ä¸­æ€§
        family_sim: ä¸å®¶åº­åœºåŸŸè¯çš„ç›¸ä¼¼åº¦
        work_sim: ä¸å·¥ä½œåœºåŸŸè¯çš„ç›¸ä¼¼åº¦
    """
    family_sim = cosine_similarity(word_vec, family_vec)
    work_sim = cosine_similarity(word_vec, work_vec)

    # åœºåŸŸåå‘åˆ†æ•° = å®¶åº­ç›¸ä¼¼åº¦ - å·¥ä½œç›¸ä¼¼åº¦
    bias_score = family_sim - work_sim

    return bias_score, family_sim, work_sim


def get_available_provinces(year):
    """è·å–æŒ‡å®šå¹´ä»½æ‰€æœ‰å¯ç”¨çš„çœä»½åˆ—è¡¨ï¼ˆä¸åŠ è½½æ¨¡å‹ï¼‰"""
    year_model_dir = os.path.join(MODEL_DIR, str(year))
    if not os.path.exists(year_model_dir):
        return []

    pattern = os.path.join(year_model_dir, "model_*.model")
    model_files = sorted(glob.glob(pattern))

    provinces = []
    for model_path in model_files:
        filename = os.path.basename(model_path)
        province = filename.replace("model_", "").replace(".model", "")
        provinces.append(province)

    return sorted(provinces)


def load_single_model(year, province):
    """åŠ è½½æŒ‡å®šå¹´ä»½å’Œçœä»½çš„å•ä¸ªæ¨¡å‹"""
    year_model_dir = os.path.join(MODEL_DIR, str(year))
    model_path = os.path.join(year_model_dir, f"model_{province}.model")

    if not os.path.exists(model_path):
        print(f"âŒ æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {model_path}")
        return None

    try:
        model = KeyedVectors.load(model_path)
        print(f"  âœ“ å·²åŠ è½½: {province} (è¯æ±‡é‡: {len(model):,})")
        return model
    except Exception as e:
        print(f"  âŒ åŠ è½½å¤±è´¥: {province} - {e}")
        return None


def load_models(year, province_filter=None):
    """åŠ è½½æŒ‡å®šå¹´ä»½çš„æ‰€æœ‰æ¨¡å‹ï¼ˆä¿ç•™æ­¤å‡½æ•°ä»¥ä¿æŒå‘åå…¼å®¹ï¼‰"""
    year_model_dir = os.path.join(MODEL_DIR, str(year))
    if not os.path.exists(year_model_dir):
        print(f"âŒ æœªæ‰¾åˆ° {year} å¹´çš„æ¨¡å‹ç›®å½•: {year_model_dir}")
        return {}

    pattern = os.path.join(year_model_dir, "model_*.model")
    model_files = sorted(glob.glob(pattern))

    if not model_files:
        print(f"âŒ æœªæ‰¾åˆ° {year} å¹´çš„æ¨¡å‹æ–‡ä»¶")
        return {}

    print(f"ğŸ“‚ æ‰¾åˆ° {len(model_files)} ä¸ªæ¨¡å‹æ–‡ä»¶")

    models = {}
    for model_path in model_files:
        # ä»æ–‡ä»¶åæå–çœä»½åç§°
        filename = os.path.basename(model_path)
        province = filename.replace("model_", "").replace(".model", "")

        # å¦‚æœæŒ‡å®šäº†çœä»½è¿‡æ»¤ï¼ŒåªåŠ è½½è¯¥çœä»½
        if province_filter and province != province_filter:
            continue

        try:
            model = KeyedVectors.load(model_path)
            models[province] = model
            print(f"  âœ“ å·²åŠ è½½: {province} (è¯æ±‡é‡: {len(model):,})")
        except Exception as e:
            print(f"  âŒ åŠ è½½å¤±è´¥: {province} - {e}")

    return models


def analyze_model(province, model):
    """åˆ†æå•ä¸ªçœä»½çš„æ¨¡å‹"""
    # åªä¿ç•™ç®€è¦çš„è¿›åº¦ä¿¡æ¯åœ¨console
    print(f"  åˆ†æçœä»½: {province}")

    # ç”¨äºæ”¶é›†è¯¦ç»†æŠ¥å‘Šçš„åˆ—è¡¨
    report_lines = []

    report_lines.append(f"\n{'='*60}")
    report_lines.append(f"çœä»½: {province}")
    report_lines.append(f"{'='*60}\n")

    vocab_size = len(model)
    report_lines.append(f"è¯æ±‡è¡¨å¤§å°: {vocab_size:,}")

    # è®¡ç®—æ€§åˆ«è¯å‘é‡
    male_vec, male_found = get_word_set_embedding(model, GENDER_WORDS["male"])
    female_vec, female_found = get_word_set_embedding(model, GENDER_WORDS["female"])

    if male_vec is None or female_vec is None:
        report_lines.append(f"âŒ æ€§åˆ«è¯å‘é‡è®¡ç®—å¤±è´¥\n")
        return None

    report_lines.append(f"æ‰¾åˆ°ç”·æ€§è¯: {len(male_found)}/{len(GENDER_WORDS['male'])} ä¸ª")
    report_lines.append(
        f"  {', '.join(male_found[:15])}{'...' if len(male_found) > 15 else ''}"
    )
    report_lines.append(
        f"æ‰¾åˆ°å¥³æ€§è¯: {len(female_found)}/{len(GENDER_WORDS['female'])} ä¸ª"
    )
    report_lines.append(
        f"  {', '.join(female_found[:15])}{'...' if len(female_found) > 15 else ''}\n"
    )

    # è®¡ç®—æ¯ä¸ªèŒä¸šè¯çš„æ€§åˆ«åå‘ï¼ˆä½¿ç”¨ä¸¤ç§æ–¹æ³•ï¼‰
    occupation_results = []
    found_occupations = []

    for occupation in ALL_OCCUPATIONS:
        occ_vec = get_word_embedding(model, occupation)
        if occ_vec is not None:
            # æ–¹æ³•1ï¼šä½™å¼¦ç›¸ä¼¼åº¦å·®å€¼
            bias_score, male_sim, female_sim = compute_gender_bias(
                occ_vec, male_vec, female_vec
            )

            # æ–¹æ³•2ï¼šæ€§åˆ«è½´æŠ•å½±
            projection_score, _ = compute_gender_bias_projection(
                occ_vec, male_vec, female_vec
            )

            occupation_results.append(
                {
                    "occupation": occupation,
                    "bias_score": float(bias_score),  # ä½™å¼¦ç›¸ä¼¼åº¦å·®å€¼æ–¹æ³•
                    "projection_score": float(projection_score),  # æ€§åˆ«è½´æŠ•å½±æ–¹æ³•
                    "male_similarity": float(male_sim),
                    "female_similarity": float(female_sim),
                }
            )
            found_occupations.append(occupation)

    if not occupation_results:
        report_lines.append(f"âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•èŒä¸šè¯\n")
        return None

    report_lines.append(
        f"æ‰¾åˆ°èŒä¸šè¯: {len(found_occupations)}/{len(ALL_OCCUPATIONS)} ä¸ª\n"
    )

    # æ’åºå¹¶å±•ç¤ºç»“æœï¼ˆæŒ‰ä½™å¼¦ç›¸ä¼¼åº¦å·®å€¼ï¼‰
    occupation_results_sorted = sorted(
        occupation_results, key=lambda x: x["bias_score"], reverse=True
    )

    report_lines.append(f"ã€èŒä¸šæ€§åˆ«åå‘åˆ†æ - ä½™å¼¦ç›¸ä¼¼åº¦å·®å€¼æ–¹æ³•ã€‘")
    report_lines.append(f"\næœ€åå¥³æ€§çš„èŒä¸š (Top 5):")
    for i, occ in enumerate(occupation_results_sorted[:5], 1):
        report_lines.append(
            f"  {i}. {occ['occupation']:8s} | åå‘åˆ†æ•°: {occ['bias_score']:+.3f} "
            f"| æŠ•å½±åˆ†æ•°: {occ['projection_score']:+.3f} "
            f"| å¥³æ€§ç›¸ä¼¼åº¦: {occ['female_similarity']:.3f} "
            f"| ç”·æ€§ç›¸ä¼¼åº¦: {occ['male_similarity']:.3f}"
        )

    report_lines.append(f"\næœ€åç”·æ€§çš„èŒä¸š (Top 5):")
    for i, occ in enumerate(occupation_results_sorted[-5:][::-1], 1):
        report_lines.append(
            f"  {i}. {occ['occupation']:8s} | åå‘åˆ†æ•°: {occ['bias_score']:+.3f} "
            f"| æŠ•å½±åˆ†æ•°: {occ['projection_score']:+.3f} "
            f"| å¥³æ€§ç›¸ä¼¼åº¦: {occ['female_similarity']:.3f} "
            f"| ç”·æ€§ç›¸ä¼¼åº¦: {occ['male_similarity']:.3f}"
        )

    # æŒ‰æŠ•å½±åˆ†æ•°æ’åºå¹¶å±•ç¤º
    occupation_results_sorted_proj = sorted(
        occupation_results, key=lambda x: x["projection_score"], reverse=True
    )

    report_lines.append(f"\nã€èŒä¸šæ€§åˆ«åå‘åˆ†æ - æ€§åˆ«è½´æŠ•å½±æ–¹æ³•ã€‘")
    report_lines.append(f"\næœ€åå¥³æ€§çš„èŒä¸š (Top 5):")
    for i, occ in enumerate(occupation_results_sorted_proj[:5], 1):
        report_lines.append(
            f"  {i}. {occ['occupation']:8s} | æŠ•å½±åˆ†æ•°: {occ['projection_score']:+.3f} "
            f"| åå‘åˆ†æ•°: {occ['bias_score']:+.3f}"
        )

    report_lines.append(f"\næœ€åç”·æ€§çš„èŒä¸š (Top 5):")
    for i, occ in enumerate(occupation_results_sorted_proj[-5:][::-1], 1):
        report_lines.append(
            f"  {i}. {occ['occupation']:8s} | æŠ•å½±åˆ†æ•°: {occ['projection_score']:+.3f} "
            f"| åå‘åˆ†æ•°: {occ['bias_score']:+.3f}"
        )

    # è®¡ç®—å®¶åŠ¡åˆ†å·¥è¯æ±‡çš„æ€§åˆ«åå‘ï¼ˆç±»ä¼¼èŒä¸šè¯åˆ†æï¼‰
    domestic_work_results = []
    found_work_words = []
    found_family_words = []

    report_lines.append(f"\nã€å®¶åŠ¡åˆ†å·¥è¯æ±‡æ€§åˆ«åå‘åˆ†æã€‘")

    # åˆ†ææ¯ä¸ªworkè¯çš„æ€§åˆ«åå‘
    for word in DOMESTIC_WORK_WORDS["work"]:
        word_vec = get_word_embedding(model, word)
        if word_vec is not None:
            # æ–¹æ³•1ï¼šä½™å¼¦ç›¸ä¼¼åº¦å·®å€¼
            bias_score, male_sim, female_sim = compute_gender_bias(
                word_vec, male_vec, female_vec
            )

            # æ–¹æ³•2ï¼šæ€§åˆ«è½´æŠ•å½±
            projection_score, _ = compute_gender_bias_projection(
                word_vec, male_vec, female_vec
            )

            domestic_work_results.append(
                {
                    "word": word,
                    "word_type": "work",
                    "bias_score": float(bias_score),
                    "projection_score": float(projection_score),
                    "male_similarity": float(male_sim),
                    "female_similarity": float(female_sim),
                }
            )
            found_work_words.append(word)

    # åˆ†ææ¯ä¸ªfamilyè¯çš„æ€§åˆ«åå‘
    for word in DOMESTIC_WORK_WORDS["family"]:
        word_vec = get_word_embedding(model, word)
        if word_vec is not None:
            # æ–¹æ³•1ï¼šä½™å¼¦ç›¸ä¼¼åº¦å·®å€¼
            bias_score, male_sim, female_sim = compute_gender_bias(
                word_vec, male_vec, female_vec
            )

            # æ–¹æ³•2ï¼šæ€§åˆ«è½´æŠ•å½±
            projection_score, _ = compute_gender_bias_projection(
                word_vec, male_vec, female_vec
            )

            domestic_work_results.append(
                {
                    "word": word,
                    "word_type": "family",
                    "bias_score": float(bias_score),
                    "projection_score": float(projection_score),
                    "male_similarity": float(male_sim),
                    "female_similarity": float(female_sim),
                }
            )
            found_family_words.append(word)

    if not domestic_work_results:
        report_lines.append(f"  (æœªæ‰¾åˆ°work/familyè¯)")
    else:
        report_lines.append(
            f"æ‰¾åˆ°workè¯: {len(found_work_words)}/{len(DOMESTIC_WORK_WORDS['work'])} ä¸ª"
        )
        report_lines.append(
            f"æ‰¾åˆ°familyè¯: {len(found_family_words)}/{len(DOMESTIC_WORK_WORDS['family'])} ä¸ª"
        )

        # åˆ†åˆ«ç»Ÿè®¡workå’Œfamilyè¯çš„æ€§åˆ«åå‘
        work_results = [r for r in domestic_work_results if r["word_type"] == "work"]
        family_results = [
            r for r in domestic_work_results if r["word_type"] == "family"
        ]

        if work_results:
            work_bias_scores = [r["bias_score"] for r in work_results]
            work_proj_scores = [r["projection_score"] for r in work_results]
            report_lines.append(f"\nWorkè¯æ±‡ç»Ÿè®¡:")
            report_lines.append(f"  ä½™å¼¦ç›¸ä¼¼åº¦å·®å€¼æ–¹æ³•:")
            report_lines.append(f"    å¹³å‡åå‘: {np.mean(work_bias_scores):+.3f}")
            report_lines.append(f"    æ ‡å‡†å·®: {np.std(work_bias_scores):.3f}")
            report_lines.append(f"  æ€§åˆ«è½´æŠ•å½±æ–¹æ³•:")
            report_lines.append(f"    å¹³å‡æŠ•å½±: {np.mean(work_proj_scores):+.3f}")
            report_lines.append(f"    æ ‡å‡†å·®: {np.std(work_proj_scores):.3f}")

        if family_results:
            family_bias_scores = [r["bias_score"] for r in family_results]
            family_proj_scores = [r["projection_score"] for r in family_results]
            report_lines.append(f"\nFamilyè¯æ±‡ç»Ÿè®¡:")
            report_lines.append(f"  ä½™å¼¦ç›¸ä¼¼åº¦å·®å€¼æ–¹æ³•:")
            report_lines.append(f"    å¹³å‡åå‘: {np.mean(family_bias_scores):+.3f}")
            report_lines.append(f"    æ ‡å‡†å·®: {np.std(family_bias_scores):.3f}")
            report_lines.append(f"  æ€§åˆ«è½´æŠ•å½±æ–¹æ³•:")
            report_lines.append(f"    å¹³å‡æŠ•å½±: {np.mean(family_proj_scores):+.3f}")
            report_lines.append(f"    æ ‡å‡†å·®: {np.std(family_proj_scores):.3f}")

        if work_results and family_results:
            bias_gap = np.mean(family_bias_scores) - np.mean(work_bias_scores)
            proj_gap = np.mean(family_proj_scores) - np.mean(work_proj_scores)
            report_lines.append(f"\nWork vs Family æ€§åˆ«å·®å¼‚:")
            report_lines.append(
                f"  ä½™å¼¦ç›¸ä¼¼åº¦å·®å€¼: {bias_gap:+.3f} (æ­£å€¼è¡¨ç¤ºfamilyæ¯”workæ›´åå¥³æ€§)"
            )
            report_lines.append(
                f"  æ€§åˆ«è½´æŠ•å½±: {proj_gap:+.3f} (æ­£å€¼è¡¨ç¤ºfamilyæ¯”workæ›´åå¥³æ€§)"
            )

        # å±•ç¤ºæœ€åå¥³æ€§å’Œæœ€åç”·æ€§çš„è¯ï¼ˆæŒ‰ä½™å¼¦ç›¸ä¼¼åº¦å·®å€¼ï¼‰
        sorted_results = sorted(
            domestic_work_results, key=lambda x: x["bias_score"], reverse=True
        )

        report_lines.append(f"\næœ€åå¥³æ€§çš„work/familyè¯ (Top 5):")
        for i, word_data in enumerate(sorted_results[:5], 1):
            report_lines.append(
                f"  {i}. [{word_data['word_type']:6s}] {word_data['word']:10s} | "
                f"åå‘åˆ†æ•°: {word_data['bias_score']:+.3f} | "
                f"æŠ•å½±åˆ†æ•°: {word_data['projection_score']:+.3f}"
            )

        report_lines.append(f"\næœ€åç”·æ€§çš„work/familyè¯ (Top 5):")
        for i, word_data in enumerate(sorted_results[-5:][::-1], 1):
            report_lines.append(
                f"  {i}. [{word_data['word_type']:6s}] {word_data['word']:10s} | "
                f"åå‘åˆ†æ•°: {word_data['bias_score']:+.3f} | "
                f"æŠ•å½±åˆ†æ•°: {word_data['projection_score']:+.3f}"
            )

    # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
    bias_scores = [r["bias_score"] for r in occupation_results]
    projection_scores = [r["projection_score"] for r in occupation_results]
    stats = {
        "province": province,
        "vocab_size": vocab_size,
        "occupations_found": len(found_occupations),
        "male_words_found": len(male_found),
        "female_words_found": len(female_found),
        # èŒä¸šè¯ï¼šä½™å¼¦ç›¸ä¼¼åº¦å·®å€¼æ–¹æ³•çš„ç»Ÿè®¡
        "occupation_mean_bias": float(np.mean(bias_scores)),
        "occupation_std_bias": float(np.std(bias_scores)),
        "occupation_min_bias": float(np.min(bias_scores)),
        "occupation_max_bias": float(np.max(bias_scores)),
        "occupation_range_bias": float(np.max(bias_scores) - np.min(bias_scores)),
        # èŒä¸šè¯ï¼šæ€§åˆ«è½´æŠ•å½±æ–¹æ³•çš„ç»Ÿè®¡
        "occupation_mean_projection": float(np.mean(projection_scores)),
        "occupation_std_projection": float(np.std(projection_scores)),
        "occupation_min_projection": float(np.min(projection_scores)),
        "occupation_max_projection": float(np.max(projection_scores)),
        "occupation_range_projection": float(
            np.max(projection_scores) - np.min(projection_scores)
        ),
    }

    # æ·»åŠ work/familyè¯æ±‡çš„ç»Ÿè®¡æŒ‡æ ‡
    if domestic_work_results:
        work_results = [r for r in domestic_work_results if r["word_type"] == "work"]
        family_results = [
            r for r in domestic_work_results if r["word_type"] == "family"
        ]

        stats["work_words_found"] = len(found_work_words)
        stats["family_words_found"] = len(found_family_words)

        if work_results:
            work_bias_scores = [r["bias_score"] for r in work_results]
            work_proj_scores = [r["projection_score"] for r in work_results]
            stats["work_mean_bias"] = float(np.mean(work_bias_scores))
            stats["work_std_bias"] = float(np.std(work_bias_scores))
            stats["work_mean_projection"] = float(np.mean(work_proj_scores))
            stats["work_std_projection"] = float(np.std(work_proj_scores))

        if family_results:
            family_bias_scores = [r["bias_score"] for r in family_results]
            family_proj_scores = [r["projection_score"] for r in family_results]
            stats["family_mean_bias"] = float(np.mean(family_bias_scores))
            stats["family_std_bias"] = float(np.std(family_bias_scores))
            stats["family_mean_projection"] = float(np.mean(family_proj_scores))
            stats["family_std_projection"] = float(np.std(family_proj_scores))

        # è®¡ç®—work vs familyçš„å·®å¼‚
        if work_results and family_results:
            stats["domain_bias_gap"] = float(
                np.mean(family_bias_scores) - np.mean(work_bias_scores)
            )
            stats["domain_projection_gap"] = float(
                np.mean(family_proj_scores) - np.mean(work_proj_scores)
            )

    report_lines.append(f"\nã€ç»Ÿè®¡æŒ‡æ ‡æ±‡æ€»ã€‘")
    report_lines.append(f"\nèŒä¸šè¯ç»Ÿè®¡:")
    report_lines.append(f"  ä½™å¼¦ç›¸ä¼¼åº¦å·®å€¼æ–¹æ³•:")
    report_lines.append(f"    å¹³å‡åå‘: {stats['occupation_mean_bias']:+.3f}")
    report_lines.append(f"    æ ‡å‡†å·®ï¼ˆéš”ç¦»ç¨‹åº¦ï¼‰: {stats['occupation_std_bias']:.3f}")
    report_lines.append(
        f"    åå‘èŒƒå›´: [{stats['occupation_min_bias']:+.3f}, {stats['occupation_max_bias']:+.3f}]"
    )
    report_lines.append(f"  æ€§åˆ«è½´æŠ•å½±æ–¹æ³•:")
    report_lines.append(f"    å¹³å‡æŠ•å½±: {stats['occupation_mean_projection']:+.3f}")
    report_lines.append(f"    æ ‡å‡†å·®: {stats['occupation_std_projection']:.3f}")
    report_lines.append(
        f"    æŠ•å½±èŒƒå›´: [{stats['occupation_min_projection']:+.3f}, {stats['occupation_max_projection']:+.3f}]"
    )

    if domestic_work_results:
        report_lines.append(f"\nWork/Familyè¯ç»Ÿè®¡:")
        if "work_mean_bias" in stats:
            report_lines.append(f"  Workè¯:")
            report_lines.append(f"    å¹³å‡åå‘: {stats['work_mean_bias']:+.3f}")
            report_lines.append(f"    å¹³å‡æŠ•å½±: {stats['work_mean_projection']:+.3f}")
        if "family_mean_bias" in stats:
            report_lines.append(f"  Familyè¯:")
            report_lines.append(f"    å¹³å‡åå‘: {stats['family_mean_bias']:+.3f}")
            report_lines.append(f"    å¹³å‡æŠ•å½±: {stats['family_mean_projection']:+.3f}")
        if "domain_bias_gap" in stats:
            report_lines.append(f"  Domainå·®å¼‚:")
            report_lines.append(f"    åå‘å·®è·: {stats['domain_bias_gap']:+.3f}")
            report_lines.append(f"    æŠ•å½±å·®è·: {stats['domain_projection_gap']:+.3f}")

    # è¿”å›åˆ†æç»“æœ
    result = {
        "province": province,
        "stats": stats,
        "male_vec": male_vec.tolist(),
        "female_vec": female_vec.tolist(),
        "male_words_found": male_found,
        "female_words_found": female_found,
        "occupations_found": found_occupations,
        "occupation_results": occupation_results,
        "domestic_work_results": domestic_work_results,
        "work_words_found": found_work_words,
        "family_words_found": found_family_words,
        "report_lines": report_lines,  # æ·»åŠ è¯¦ç»†æŠ¥å‘Š
    }

    return result


def analyze_all_models(models):
    """åˆ†ææ‰€æœ‰çœä»½çš„æ¨¡å‹"""
    results = []
    province_stats = []

    for province, model in models.items():
        result = analyze_model(province, model)
        if result:
            results.append(result)
            province_stats.append(result["stats"])

    return results, province_stats


def save_results(results, province_stats, year):
    """ä¿å­˜åˆ†æç»“æœ"""
    if not results:
        print("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•ç»“æœ")
        return

    year_output_dir = os.path.join(OUTPUT_DIR, str(year))
    os.makedirs(year_output_dir, exist_ok=True)

    print(f"\n{'='*60}")
    print(f"ğŸ’¾ ä¿å­˜ç»“æœ...")
    print(f"{'='*60}")

    # 1. ä¿å­˜çœä»½ç»Ÿè®¡ä¿¡æ¯
    stats_df = pd.DataFrame(province_stats)
    stats_file = os.path.join(year_output_dir, f"province_stats.csv")
    stats_df.to_csv(stats_file, index=False, encoding="utf-8-sig")
    print(f"âœ“ çœä»½ç»Ÿè®¡ä¿¡æ¯: {stats_file}")

    # 2. ä¿å­˜èŒä¸šæ€§åˆ«åå‘è¯¦ç»†æ•°æ®ï¼ˆé•¿æ ¼å¼ï¼‰
    occupation_data = []
    for result in results:
        province = result["province"]
        for occ in result["occupation_results"]:
            occupation_data.append(
                {
                    "province": province,
                    "occupation": occ["occupation"],
                    "bias_score": occ["bias_score"],  # ä½™å¼¦ç›¸ä¼¼åº¦å·®å€¼æ–¹æ³•
                    "projection_score": occ["projection_score"],  # æ€§åˆ«è½´æŠ•å½±æ–¹æ³•
                    "male_similarity": occ["male_similarity"],
                    "female_similarity": occ["female_similarity"],
                }
            )

    occupation_df = pd.DataFrame(occupation_data)
    occupation_file = os.path.join(year_output_dir, f"occupation_bias.csv")
    occupation_df.to_csv(occupation_file, index=False, encoding="utf-8-sig")
    print(f"âœ“ èŒä¸šæ€§åˆ«åå‘æ•°æ®: {occupation_file}")

    # 2.5. ä¿å­˜work/familyè¯æ±‡æ€§åˆ«åå‘æ•°æ®ï¼ˆç±»ä¼¼èŒä¸šæ•°æ®ï¼‰
    domestic_work_data = []
    for result in results:
        province = result["province"]
        if result.get("domestic_work_results"):
            for dw in result["domestic_work_results"]:
                domestic_work_data.append(
                    {
                        "province": province,
                        "word": dw["word"],
                        "word_type": dw["word_type"],
                        "bias_score": dw["bias_score"],
                        "projection_score": dw["projection_score"],
                        "male_similarity": dw["male_similarity"],
                        "female_similarity": dw["female_similarity"],
                    }
                )

    if domestic_work_data:
        domestic_work_df = pd.DataFrame(domestic_work_data)
        domestic_work_file = os.path.join(year_output_dir, f"domestic_work_bias.csv")
        domestic_work_df.to_csv(domestic_work_file, index=False, encoding="utf-8-sig")
        print(f"âœ“ Work/Familyè¯æ±‡æ€§åˆ«åå‘æ•°æ®: {domestic_work_file}")

        # ä¿å­˜å®½æ ¼å¼æ•°æ®ï¼ˆçœä»½Ã—è¯æ±‡çŸ©é˜µï¼‰
        # 2.5.1 ä½™å¼¦ç›¸ä¼¼åº¦å·®å€¼æ–¹æ³•çš„çŸ©é˜µ
        dw_pivot_df = domestic_work_df.pivot_table(
            values="bias_score", index="word", columns="province", aggfunc="mean"
        )
        dw_pivot_file = os.path.join(year_output_dir, f"domestic_work_bias_pivot.csv")
        dw_pivot_df.to_csv(dw_pivot_file, encoding="utf-8-sig")
        print(f"âœ“ Work/Familyè¯Ã—çœä»½çŸ©é˜µï¼ˆä½™å¼¦ç›¸ä¼¼åº¦å·®å€¼ï¼‰: {dw_pivot_file}")

        # 2.5.2 æ€§åˆ«è½´æŠ•å½±æ–¹æ³•çš„çŸ©é˜µ
        dw_pivot_proj_df = domestic_work_df.pivot_table(
            values="projection_score", index="word", columns="province", aggfunc="mean"
        )
        dw_pivot_proj_file = os.path.join(
            year_output_dir, f"domestic_work_projection_pivot.csv"
        )
        dw_pivot_proj_df.to_csv(dw_pivot_proj_file, encoding="utf-8-sig")
        print(f"âœ“ Work/Familyè¯Ã—çœä»½çŸ©é˜µï¼ˆæ€§åˆ«è½´æŠ•å½±ï¼‰: {dw_pivot_proj_file}")

    # 3. ä¿å­˜å®½æ ¼å¼æ•°æ®ï¼ˆçœä»½Ã—èŒä¸šçŸ©é˜µï¼‰
    # 3.1 ä½™å¼¦ç›¸ä¼¼åº¦å·®å€¼æ–¹æ³•çš„çŸ©é˜µ
    pivot_df = occupation_df.pivot_table(
        values="bias_score", index="occupation", columns="province", aggfunc="mean"
    )
    pivot_file = os.path.join(year_output_dir, f"occupation_bias_pivot.csv")
    pivot_df.to_csv(pivot_file, encoding="utf-8-sig")
    print(f"âœ“ èŒä¸šÃ—çœä»½çŸ©é˜µï¼ˆä½™å¼¦ç›¸ä¼¼åº¦å·®å€¼ï¼‰: {pivot_file}")

    # 3.2 æ€§åˆ«è½´æŠ•å½±æ–¹æ³•çš„çŸ©é˜µ
    pivot_proj_df = occupation_df.pivot_table(
        values="projection_score",
        index="occupation",
        columns="province",
        aggfunc="mean",
    )
    pivot_proj_file = os.path.join(year_output_dir, f"occupation_projection_pivot.csv")
    pivot_proj_df.to_csv(pivot_proj_file, encoding="utf-8-sig")
    print(f"âœ“ èŒä¸šÃ—çœä»½çŸ©é˜µï¼ˆæ€§åˆ«è½´æŠ•å½±ï¼‰: {pivot_proj_file}")

    # 4. ä¿å­˜è¯¦ç»†å‘é‡æ•°æ®ï¼ˆJSONæ ¼å¼ï¼‰
    detailed_data = []
    for result in results:
        detailed_data.append(
            {
                "province": result["province"],
                "stats": result["stats"],
                "male_vec": result["male_vec"],
                "female_vec": result["female_vec"],
                "male_words_found": result["male_words_found"],
                "female_words_found": result["female_words_found"],
                "occupations_found": result["occupations_found"],
            }
        )

    detailed_file = os.path.join(year_output_dir, f"detailed_vectors.json")
    with open(detailed_file, "w", encoding="utf-8") as f:
        json.dump(detailed_data, f, ensure_ascii=False, indent=2)
    print(f"âœ“ è¯¦ç»†å‘é‡æ•°æ®: {detailed_file}")

    # 5. ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š
    report_file = os.path.join(year_output_dir, f"analysis_report.txt")
    with open(report_file, "w", encoding="utf-8") as f:
        f.write(f"{'='*80}\n")
        f.write(f"æ€§åˆ«-èŒä¸šEmbeddingåˆ†ææŠ¥å‘Š ({year}å¹´)\n")
        f.write(f"{'='*80}\n\n")

        f.write(f"åˆ†æçœä»½æ•°: {len(results)}\n")
        f.write(f"åˆ†æèŒä¸šæ•°: {len(ALL_OCCUPATIONS)}\n\n")

        # å†™å…¥æ¯ä¸ªçœä»½çš„è¯¦ç»†åˆ†ææŠ¥å‘Š
        f.write(f"\n{'#'*80}\n")
        f.write(f"# å„çœä»½è¯¦ç»†åˆ†æ\n")
        f.write(f"{'#'*80}\n")

        for result in results:
            if "report_lines" in result:
                f.write("\n")
                for line in result["report_lines"]:
                    f.write(f"{line}\n")

        # åˆ†éš”ç¬¦
        f.write(f"\n\n{'#'*80}\n")
        f.write(f"# è·¨çœä»½æ±‡æ€»ç»Ÿè®¡\n")
        f.write(f"{'#'*80}\n\n")

        f.write(f"{'='*60}\n")
        f.write(f"å„çœä»½èŒä¸šæ€§åˆ«éš”ç¦»æŒ‡æ•°æ’åï¼ˆä½™å¼¦ç›¸ä¼¼åº¦å·®å€¼æ–¹æ³•ï¼ŒæŒ‰æ ‡å‡†å·®ï¼‰:\n")
        f.write(f"{'='*60}\n")
        stats_sorted = sorted(
            province_stats, key=lambda x: x["occupation_std_bias"], reverse=True
        )
        for i, stat in enumerate(stats_sorted, 1):
            f.write(
                f"{i:2d}. {stat['province']:10s} | "
                f"éš”ç¦»æŒ‡æ•°: {stat['occupation_std_bias']:.3f} | "
                f"å¹³å‡åå‘: {stat['occupation_mean_bias']:+.3f}\n"
            )

        f.write(f"\n{'='*60}\n")
        f.write(f"å„çœä»½èŒä¸šæ€§åˆ«éš”ç¦»æŒ‡æ•°æ’åï¼ˆæ€§åˆ«è½´æŠ•å½±æ–¹æ³•ï¼ŒæŒ‰æ ‡å‡†å·®ï¼‰:\n")
        f.write(f"{'='*60}\n")
        stats_sorted_proj = sorted(
            province_stats,
            key=lambda x: x.get("occupation_std_projection", 0),
            reverse=True,
        )
        for i, stat in enumerate(stats_sorted_proj, 1):
            f.write(
                f"{i:2d}. {stat['province']:10s} | "
                f"éš”ç¦»æŒ‡æ•°: {stat.get('occupation_std_projection', 0):.3f} | "
                f"å¹³å‡æŠ•å½±: {stat.get('occupation_mean_projection', 0):+.3f}\n"
            )

        f.write(f"\n{'='*60}\n")
        f.write(f"èŒä¸šæ€§åˆ«åå‘ä¸€è‡´æ€§åˆ†æ:\n")
        f.write(f"{'='*60}\n")

        # è®¡ç®—æ¯ä¸ªèŒä¸šåœ¨å„çœä»½çš„å¹³å‡åå‘
        occupation_avg = (
            occupation_df.groupby("occupation")["bias_score"]
            .agg(["mean", "std"])
            .sort_values("mean", ascending=False)
        )

        f.write(f"\næœ€åå¥³æ€§çš„èŒä¸šï¼ˆè·¨çœä»½å¹³å‡ï¼‰:\n")
        for i, (occ, row) in enumerate(occupation_avg.head(10).iterrows(), 1):
            f.write(
                f"  {i:2d}. {occ:15s} | å¹³å‡: {row['mean']:+.3f} | æ ‡å‡†å·®: {row['std']:.3f}\n"
            )

        f.write(f"\næœ€åç”·æ€§çš„èŒä¸šï¼ˆè·¨çœä»½å¹³å‡ï¼‰:\n")
        for i, (occ, row) in enumerate(
            occupation_avg.tail(10).iloc[::-1].iterrows(), 1
        ):
            f.write(
                f"  {i:2d}. {occ:15s} | å¹³å‡: {row['mean']:+.3f} | æ ‡å‡†å·®: {row['std']:.3f}\n"
            )

        f.write(f"\nèŒä¸šåå‘å·®å¼‚æœ€å¤§çš„ï¼ˆè·¨çœä»½æ ‡å‡†å·®æœ€å¤§ï¼‰:\n")
        occupation_var = occupation_avg.sort_values("std", ascending=False)
        for i, (occ, row) in enumerate(occupation_var.head(10).iterrows(), 1):
            f.write(
                f"  {i:2d}. {occ:15s} | å¹³å‡: {row['mean']:+.3f} | æ ‡å‡†å·®: {row['std']:.3f}\n"
            )

        # æ·»åŠ æ€§åˆ«è½´æŠ•å½±æ–¹æ³•åˆ†æ
        f.write(f"\n{'='*60}\n")
        f.write(f"èŒä¸šæ€§åˆ«åå‘åˆ†æï¼ˆæ€§åˆ«è½´æŠ•å½±æ–¹æ³•ï¼‰:\n")
        f.write(f"{'='*60}\n")

        # è®¡ç®—æ¯ä¸ªèŒä¸šåœ¨å„çœä»½çš„å¹³å‡æŠ•å½±åˆ†æ•°
        occupation_proj_avg = (
            occupation_df.groupby("occupation")["projection_score"]
            .agg(["mean", "std"])
            .sort_values("mean", ascending=False)
        )

        f.write(f"\næœ€åå¥³æ€§çš„èŒä¸šï¼ˆè·¨çœä»½å¹³å‡ï¼ŒæŒ‰æŠ•å½±åˆ†æ•°ï¼‰:\n")
        for i, (occ, row) in enumerate(occupation_proj_avg.head(10).iterrows(), 1):
            f.write(
                f"  {i:2d}. {occ:15s} | å¹³å‡æŠ•å½±: {row['mean']:+.3f} | æ ‡å‡†å·®: {row['std']:.3f}\n"
            )

        f.write(f"\næœ€åç”·æ€§çš„èŒä¸šï¼ˆè·¨çœä»½å¹³å‡ï¼ŒæŒ‰æŠ•å½±åˆ†æ•°ï¼‰:\n")
        for i, (occ, row) in enumerate(
            occupation_proj_avg.tail(10).iloc[::-1].iterrows(), 1
        ):
            f.write(
                f"  {i:2d}. {occ:15s} | å¹³å‡æŠ•å½±: {row['mean']:+.3f} | æ ‡å‡†å·®: {row['std']:.3f}\n"
            )

        f.write(f"\nèŒä¸šæŠ•å½±å·®å¼‚æœ€å¤§çš„ï¼ˆè·¨çœä»½æ ‡å‡†å·®æœ€å¤§ï¼‰:\n")
        occupation_proj_var = occupation_proj_avg.sort_values("std", ascending=False)
        for i, (occ, row) in enumerate(occupation_proj_var.head(10).iterrows(), 1):
            f.write(
                f"  {i:2d}. {occ:15s} | å¹³å‡æŠ•å½±: {row['mean']:+.3f} | æ ‡å‡†å·®: {row['std']:.3f}\n"
            )

        # æ·»åŠ work/familyè¯æ±‡æ€§åˆ«åå‘åˆ†æ
        if domestic_work_data:
            f.write(f"\n{'='*60}\n")
            f.write(f"Work/Familyè¯æ±‡æ€§åˆ«åå‘åˆ†æ:\n")
            f.write(f"{'='*60}\n")

            domestic_work_df = pd.DataFrame(domestic_work_data)

            # å„çœä»½work vs familyå¹³å‡åå‘å·®å¼‚æ’å
            f.write(f"\nå„çœä»½Work vs Familyæ€§åˆ«å·®å¼‚æ’åï¼ˆä½™å¼¦ç›¸ä¼¼åº¦å·®å€¼æ–¹æ³•ï¼‰:\n")
            f.write(f"(æ­£å€¼è¡¨ç¤ºfamilyæ¯”workæ›´åå¥³æ€§)\n")
            province_gaps = []
            for stat in province_stats:
                if "domain_bias_gap" in stat:
                    province_gaps.append(
                        {
                            "province": stat["province"],
                            "gap": stat["domain_bias_gap"],
                        }
                    )
            if province_gaps:
                province_gaps_df = pd.DataFrame(province_gaps)
                province_gaps_df = province_gaps_df.sort_values("gap", ascending=False)
                for i, row in enumerate(province_gaps_df.itertuples(), 1):
                    f.write(f"  {i:2d}. {row.province:10s} | å·®å¼‚: {row.gap:+.3f}\n")

            # å„çœä»½work vs familyå¹³å‡æŠ•å½±å·®å¼‚æ’å
            f.write(f"\nå„çœä»½Work vs Familyæ€§åˆ«å·®å¼‚æ’åï¼ˆæ€§åˆ«è½´æŠ•å½±æ–¹æ³•ï¼‰:\n")
            f.write(f"(æ­£å€¼è¡¨ç¤ºfamilyæ¯”workæ›´åå¥³æ€§)\n")
            province_proj_gaps = []
            for stat in province_stats:
                if "domain_projection_gap" in stat:
                    province_proj_gaps.append(
                        {
                            "province": stat["province"],
                            "gap": stat["domain_projection_gap"],
                        }
                    )
            if province_proj_gaps:
                province_proj_gaps_df = pd.DataFrame(province_proj_gaps)
                province_proj_gaps_df = province_proj_gaps_df.sort_values(
                    "gap", ascending=False
                )
                for i, row in enumerate(province_proj_gaps_df.itertuples(), 1):
                    f.write(f"  {i:2d}. {row.province:10s} | å·®å¼‚: {row.gap:+.3f}\n")

            # Work/Familyè¯æ±‡ä¸€è‡´æ€§åˆ†æ
            f.write(f"\n{'='*60}\n")
            f.write(f"Work/Familyè¯æ±‡æ€§åˆ«åå‘ä¸€è‡´æ€§åˆ†æ:\n")
            f.write(f"{'='*60}\n")

            # è®¡ç®—æ¯ä¸ªè¯æ±‡åœ¨å„çœä»½çš„å¹³å‡åå‘
            word_avg = (
                domestic_work_df.groupby("word")["bias_score"]
                .agg(["mean", "std"])
                .sort_values("mean", ascending=False)
            )

            f.write(f"\næœ€åå¥³æ€§çš„work/familyè¯ï¼ˆè·¨çœä»½å¹³å‡ï¼‰:\n")
            for i, (word, row) in enumerate(word_avg.head(10).iterrows(), 1):
                word_type = domestic_work_df[domestic_work_df["word"] == word][
                    "word_type"
                ].iloc[0]
                f.write(
                    f"  {i:2d}. [{word_type:6s}] {word:15s} | å¹³å‡: {row['mean']:+.3f} | æ ‡å‡†å·®: {row['std']:.3f}\n"
                )

            f.write(f"\næœ€åç”·æ€§çš„work/familyè¯ï¼ˆè·¨çœä»½å¹³å‡ï¼‰:\n")
            for i, (word, row) in enumerate(word_avg.tail(10).iloc[::-1].iterrows(), 1):
                word_type = domestic_work_df[domestic_work_df["word"] == word][
                    "word_type"
                ].iloc[0]
                f.write(
                    f"  {i:2d}. [{word_type:6s}] {word:15s} | å¹³å‡: {row['mean']:+.3f} | æ ‡å‡†å·®: {row['std']:.3f}\n"
                )

            # åˆ†åˆ«ç»Ÿè®¡workå’Œfamilyè¯æ±‡çš„è·¨çœä»½å¹³å‡
            work_df = domestic_work_df[domestic_work_df["word_type"] == "work"]
            family_df = domestic_work_df[domestic_work_df["word_type"] == "family"]

            if not work_df.empty and not family_df.empty:
                work_mean = work_df["bias_score"].mean()
                family_mean = family_df["bias_score"].mean()
                work_proj_mean = work_df["projection_score"].mean()
                family_proj_mean = family_df["projection_score"].mean()

                f.write(f"\nè·¨çœä»½å¹³å‡æ€§åˆ«åå‘:\n")
                f.write(f"  ä½™å¼¦ç›¸ä¼¼åº¦å·®å€¼æ–¹æ³•:\n")
                f.write(f"    Workè¯æ±‡: {work_mean:+.3f}\n")
                f.write(f"    Familyè¯æ±‡: {family_mean:+.3f}\n")
                f.write(f"    å·®å¼‚: {family_mean - work_mean:+.3f}\n")
                f.write(f"  æ€§åˆ«è½´æŠ•å½±æ–¹æ³•:\n")
                f.write(f"    Workè¯æ±‡: {work_proj_mean:+.3f}\n")
                f.write(f"    Familyè¯æ±‡: {family_proj_mean:+.3f}\n")
                f.write(f"    å·®å¼‚: {family_proj_mean - work_proj_mean:+.3f}\n")

    print(f"âœ“ åˆ†ææŠ¥å‘Š: {report_file}")

    print(f"\nâœ… æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ° {year_output_dir}/ ç›®å½•")


def main(year: int, province: str = None):
    """
    è¿è¡Œembeddingåˆ†æ

    Args:
        year: å¹´ä»½
        province: æŒ‡å®šçœä»½ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™åˆ†ææ‰€æœ‰çœä»½
    """
    print(f"\n{'='*60}")
    print(f"ğŸš€ å¼€å§‹åˆ†æ {year} å¹´æ•°æ®çš„æ€§åˆ«-èŒä¸šEmbedding")
    print(f"{'='*60}\n")

    # è·å–è¦åˆ†æçš„çœä»½åˆ—è¡¨
    if province:
        provinces_to_analyze = [province]
        print(f"ğŸ¯ åˆ†ææŒ‡å®šçœä»½: {province}\n")
    else:
        provinces_to_analyze = get_available_provinces(year)
        if not provinces_to_analyze:
            print(f"âŒ æœªæ‰¾åˆ° {year} å¹´çš„æ¨¡å‹æ–‡ä»¶")
            return
        print(f"ğŸ“‚ æ‰¾åˆ° {len(provinces_to_analyze)} ä¸ªçœä»½ï¼Œå°†é€ä¸ªåˆ†æ\n")

    # é€ä¸ªåŠ è½½å’Œåˆ†æçœä»½æ¨¡å‹ï¼ˆèŠ‚çœå†…å­˜ï¼‰
    results = []
    province_stats = []

    for idx, province_name in enumerate(provinces_to_analyze, 1):
        print(f"\n{'='*60}")
        print(f"å¤„ç†è¿›åº¦: [{idx}/{len(provinces_to_analyze)}] {province_name}")
        print(f"{'='*60}")

        # åŠ è½½å•ä¸ªæ¨¡å‹
        model = load_single_model(year, province_name)
        if model is None:
            print(f"  âš ï¸  è·³è¿‡çœä»½: {province_name}")
            continue

        # åˆ†æå•ä¸ªæ¨¡å‹
        result = analyze_model(province_name, model)
        if result:
            results.append(result)
            province_stats.append(result["stats"])

        # é‡Šæ”¾æ¨¡å‹å†…å­˜
        del model
        import gc

        gc.collect()

    if not results:
        print("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•ç»“æœ")
        return

    # ä¿å­˜ç»“æœ
    save_results(results, province_stats, year)

    print(f"\n{'='*60}")
    print(f"ğŸ‰ {year} å¹´embeddingåˆ†æå®Œæˆï¼å…±åˆ†æ {len(results)} ä¸ªçœä»½")
    print(f"{'='*60}\n")


if __name__ == "__main__":
    fire.Fire(main)
