"""
æ„å»ºå¨±ä¹è´¦å·è¯æ±‡è¡¨
ä»æŒ‡å®šå¹´ä»½çƒ­æœæ¦œå•æ•°æ®ä¸­æå–åè¯ï¼ˆè¦†ç›–æ˜æ˜Ÿã€å½±è§†å‰§åã€äº‹ä»¶ç­‰ï¼‰

åŠŸèƒ½è¯´æ˜:
1. ä»bangdanæ•°æ®ä¸­æå–çƒ­æœè¯
2. è¿‡æ»¤å¹¿å‘Šï¼ˆæ£€æŸ¥actionlog.extä¸­çš„ads_wordå­—æ®µï¼‰
3. å¯¹çƒ­æœè¯å»é‡
4. ä½¿ç”¨jiebaåˆ†è¯+è¯æ€§æ ‡æ³¨ï¼Œæå–2-4ä¸ªå­—ç¬¦çš„åè¯
5. æŒ‰é¢‘ç‡æ’åºï¼Œè¾“å‡ºå‰Nä¸ªé«˜é¢‘åè¯åˆ°txtæ–‡ä»¶ï¼ˆä¸€è¡Œä¸€ä¸ªè¯ï¼‰
6. ç”¨æˆ·å¯è¿›ä¸€æ­¥æ‰‹å·¥ç­›é€‰å¾—åˆ°å¨±ä¹ç›¸å…³è¯æ±‡

ä½¿ç”¨æ–¹æ³•:
---------
1. æ¢ç´¢æ•°æ®ç»“æ„ï¼ˆè§£å‹å¹¶æŸ¥çœ‹ç¬¬ä¸€ä¸ªæ–‡ä»¶ï¼‰:
   python build_entertain_vocab.py explore --year=2020

2. æ„å»ºå¨±ä¹è¯æ±‡è¡¨ï¼ˆä¸»è¦åŠŸèƒ½ï¼‰:
   python build_entertain_vocab.py build --year=2020
   python build_entertain_vocab.py build --year=2020 --top_n=3000
   python build_entertain_vocab.py build --year=2020 --output_file=wordlists/my_nouns.txt

å‚æ•°è¯´æ˜:
---------
- year: å¹´ä»½ï¼ˆå¿…éœ€ï¼‰
- top_n: è¾“å‡ºå‰Nä¸ªé«˜é¢‘è¯ï¼Œé»˜è®¤5000
- output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º wordlists/entertainment_nouns_{year}.txt

ä¾èµ–å®‰è£…:
---------
éœ€è¦å®‰è£… jiebaï¼ˆä¸­æ–‡åˆ†è¯å·¥å…·ï¼‰:
  pip install jieba

è¾“å‡ºæ ¼å¼:
---------
è¾“å‡ºæ–‡ä»¶æ¯è¡Œä¸€ä¸ªåè¯ï¼ŒæŒ‰é¢‘ç‡ä»é«˜åˆ°ä½æ’åº
åŒ…æ‹¬ä½†ä¸é™äºï¼šäººåã€å½±è§†å‰§åã€äº‹ä»¶åã€åœ°ç‚¹ç­‰
ä¾‹å¦‚:
  ç‹ä¸€åš
  ä¸‰åè€Œå·²
  èµµä¸½é¢–
  æ¼”å”±ä¼š
  é‡‘é¹°å¥–
  ...

ç”¨æˆ·å¯æ ¹æ®è¾“å‡ºç»“æœè¿›ä¸€æ­¥ç­›é€‰å¨±ä¹ç›¸å…³è¯æ±‡
"""

import os
import re
import json
from datetime import datetime, timedelta
from collections import Counter, defaultdict
import fire

from configs.configs import ORIGIN_DATA_DIR
from utils.utils import extract_single_7z_file, extract_7z_files

# å¯¼å…¥jiebaè¿›è¡Œåˆ†è¯å’Œè¯æ€§æ ‡æ³¨
try:
    import jieba.posseg as pseg

    print("âœ“ ä½¿ç”¨ jieba è¿›è¡Œä¸­æ–‡åˆ†è¯å’Œè¯æ€§æ ‡æ³¨")
    JIEBA_AVAILABLE = True
except ImportError:
    print("âŒ æœªå®‰è£… jiebaï¼Œè¯·è¿è¡Œ: pip install jieba")
    JIEBA_AVAILABLE = False


def get_bangdan_files_dir(year):
    return f"{ORIGIN_DATA_DIR}/{year}/bangdan/"


def get_bangdan_unzipped_files_dir(year):
    return f"bangdan_data/{year}/"


def unzip_all_bangdan_files(year):
    """
    å°†åŸå§‹å¾®åšæ•°æ®è§£å‹ç¼©åˆ°å½“å‰ç›®å½•çš„bangdan_dataæ–‡ä»¶å¤¹
    """
    bangdan_files_dir = get_bangdan_files_dir(year)
    unzipped_dir = get_bangdan_unzipped_files_dir(year)
    extract_7z_files(source_folder=bangdan_files_dir, target_folder=unzipped_dir)
    return True


def extract_nouns(text):
    """
    ä»æ–‡æœ¬ä¸­æå–2-4ä¸ªå­—ç¬¦çš„åè¯

    ä½¿ç”¨jiebaåˆ†è¯+è¯æ€§æ ‡æ³¨ï¼Œæå–æ‰€æœ‰åè¯ç±»è¯æ±‡
    åŒ…æ‹¬ï¼šäººåã€åœ°åã€æœºæ„åã€ä½œå“åç­‰

    Args:
        text: è¾“å…¥æ–‡æœ¬

    Returns:
        list: æå–åˆ°çš„åè¯åˆ—è¡¨
    """
    if not JIEBA_AVAILABLE:
        print("âŒ jiebaæœªå®‰è£…ï¼Œæ— æ³•æå–åè¯")
        return []

    nouns = []

    try:
        # ä½¿ç”¨jiebaè¿›è¡Œåˆ†è¯å’Œè¯æ€§æ ‡æ³¨
        words = pseg.cut(text)

        for word, flag in words:
            # æå–åè¯ç±»è¯æ±‡
            # jiebaè¯æ€§æ ‡æ³¨ä¸­ï¼Œä»¥'n'å¼€å¤´çš„éƒ½æ˜¯åè¯ï¼š
            # - n: æ™®é€šåè¯
            # - nr: äººå
            # - nz: å…¶ä»–ä¸“æœ‰åè¯
            # - ns: åœ°å
            # - nt: æœºæ„å›¢ä½“å
            # - nw: ä½œå“å
            # - nrfg: äººå (å¤åˆ)
            if flag in ["nr", "nrfg", "nw"] and 2 <= len(word) <= 4:
                # ç¡®ä¿æ˜¯ä¸­æ–‡å­—ç¬¦
                if all("\u4e00" <= char <= "\u9fff" for char in word):
                    nouns.append(word)
    except Exception as e:
        print(f"jiebaå¤„ç†å‡ºé”™: {e}")

    return nouns


def is_advertisement(actionlog_ext):
    """
    åˆ¤æ–­æ˜¯å¦ä¸ºå¹¿å‘Š

    Args:
        actionlog_ext: actionlogä¸­çš„extå­—æ®µ

    Returns:
        bool: Trueè¡¨ç¤ºæ˜¯å¹¿å‘Š
    """
    if actionlog_ext and "ads_word" in actionlog_ext:
        return True
    return False


def extract_hotwords_from_bangdan_file(file_path, verbose=False):
    """
    ä»å•ä¸ªbangdanæ–‡ä»¶ä¸­æå–çƒ­æœè¯ï¼ˆè¿‡æ»¤å¹¿å‘Šï¼Œæå–åè¯ï¼‰

    Args:
        file_path: bangdanæ–‡ä»¶è·¯å¾„
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯

    Returns:
        list: æå–åˆ°çš„åè¯åˆ—è¡¨
    """
    nouns_list = []
    hotwords_set = set()  # ç”¨äºå»é‡çƒ­æœè¯
    ad_count = 0
    valid_count = 0

    if not os.path.exists(file_path):
        if verbose:
            print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return []

    with open(file_path, "r", errors="replace") as rfile:
        for line in rfile.readlines():
            line = line.strip()
            if not line:
                continue

            line_data = line.split("\t")
            if len(line_data) < 2:
                continue

            try:
                data = json.loads(line_data[1])
            except json.JSONDecodeError:
                continue

            # è§£æbangdanæ•°æ®
            if "bangdan" not in data:
                continue

            try:
                bangdan_data = json.loads(data["bangdan"])
            except (json.JSONDecodeError, TypeError):
                continue

            if type(bangdan_data) is not dict:
                continue

            if "cards" not in bangdan_data or bangdan_data["cards"] is None:
                continue

            # éå†æ‰€æœ‰card
            for card in bangdan_data["cards"]:
                if str(card.get("card_type")) != "11":
                    continue

                card_group = card.get("card_group", [])
                for s_card in card_group:
                    if str(s_card.get("card_type")) != "4":
                        continue

                    # æ£€æŸ¥æ˜¯å¦ä¸ºå¹¿å‘Š
                    actionlog = s_card.get("actionlog", {})
                    actionlog_ext = actionlog.get("ext", "")

                    if is_advertisement(actionlog_ext):
                        ad_count += 1
                        continue

                    # æå–descå­—æ®µ
                    desc = s_card.get("desc", "")
                    if not desc or len(desc) <= 1:
                        continue

                    # å»é‡
                    if desc in hotwords_set:
                        continue
                    hotwords_set.add(desc)

                    valid_count += 1

                    # æå–åè¯
                    nouns = extract_nouns(desc)
                    nouns_list.extend(nouns)

    if verbose:
        print(f"  æ–‡ä»¶: {os.path.basename(file_path)}")
        print(
            f"    æœ‰æ•ˆçƒ­æœ: {valid_count}, è¿‡æ»¤å¹¿å‘Š: {ad_count}, æå–åè¯: {len(nouns_list)}"
        )

    return nouns_list


def explore_bangdan_data(year: int):
    """
    æ¢ç´¢bangdanæ•°æ®ç»“æ„
    è§£å‹ä¸€ä¸ªæ–‡ä»¶ï¼Œæ‰“å°cardlistInfoå’Œcardsçš„å‰10ä¸ªå…ƒç´ 
    """
    bangdan_files_dir = get_bangdan_files_dir(year)
    unzipped_dir = get_bangdan_unzipped_files_dir(year)

    # ç¡®ä¿ç›®æ ‡æ–‡ä»¶å¤¹å­˜åœ¨
    if not os.path.exists(unzipped_dir):
        os.makedirs(unzipped_dir)

    # æ‰¾åˆ°ç¬¬ä¸€ä¸ª.7zæ–‡ä»¶
    if not os.path.exists(bangdan_files_dir):
        print(f"ç›®å½•ä¸å­˜åœ¨: {bangdan_files_dir}")
        return

    # è·å–æ‰€æœ‰.7zæ–‡ä»¶
    zip_files = [f for f in os.listdir(bangdan_files_dir) if f.endswith(".7z")]
    if not zip_files:
        print(f"åœ¨ {bangdan_files_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°.7zæ–‡ä»¶")
        return

    # è§£å‹ç¬¬ä¸€ä¸ªæ–‡ä»¶
    first_zip_file = os.path.join(bangdan_files_dir, zip_files[0])
    print(f"æ­£åœ¨è§£å‹æ–‡ä»¶: {first_zip_file}")
    result = extract_single_7z_file(
        file_path=first_zip_file, target_folder=unzipped_dir
    )

    if result != "success":
        print("è§£å‹å¤±è´¥")
        return

    # æ‰¾åˆ°è§£å‹åçš„æ–‡ä»¶
    unzipped_files = [f for f in os.listdir(unzipped_dir) if not f.endswith(".7z")]
    if not unzipped_files:
        print(f"è§£å‹åæ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶åœ¨ {unzipped_dir}")
        return

    # è¯»å–ç¬¬ä¸€ä¸ªæ–‡ä»¶çš„ç¬¬ä¸€è¡Œæœ‰æ•ˆæ•°æ®
    first_file = os.path.join(unzipped_dir, unzipped_files[0])
    print(f"\næ­£åœ¨è¯»å–æ–‡ä»¶: {first_file}")

    with open(first_file, "r", errors="replace") as rfile:
        for line_num, line in enumerate(rfile.readlines(), 1):
            line = line.strip()
            if not line:
                continue

            line_data = line.split("\t")
            if len(line_data) < 2:
                continue

            try:
                data = json.loads(line_data[1])
            except json.JSONDecodeError as e:
                continue

            # è§£æbangdanæ•°æ®
            if "bangdan" not in data:
                continue

            try:
                bangdan_data = json.loads(data["bangdan"])
            except (json.JSONDecodeError, TypeError):
                continue

            if type(bangdan_data) is not dict:
                continue

            print(f"\n{'='*80}")
            print(f"æ‰¾åˆ°æœ‰æ•ˆæ•°æ® (ç¬¬ {line_num} è¡Œ)")
            print(f"{'='*80}\n")

            # æ‰“å°cardlistInfo
            if "cardlistInfo" in bangdan_data:
                print("=" * 80)
                print("cardlistInfo æ•°æ®æ ¼å¼:")
                print("=" * 80)
                print(
                    json.dumps(
                        bangdan_data["cardlistInfo"], ensure_ascii=False, indent=2
                    )
                )
                print()
            else:
                print("æ³¨æ„: bangdan_data ä¸­æ²¡æœ‰ 'cardlistInfo' å­—æ®µ")
                print(f"bangdan_data çš„é”®: {list(bangdan_data.keys())}")
                print()

            # æ‰“å°cardsçš„å‰10ä¸ªå…ƒç´ 
            if "cards" in bangdan_data and bangdan_data["cards"]:
                print("=" * 80)
                print(
                    f"cards æ•°æ®æ ¼å¼ (å‰10ä¸ªå…ƒç´ ï¼Œå…± {len(bangdan_data['cards'])} ä¸ª):"
                )
                print("=" * 80)
                for i, card in enumerate(bangdan_data["cards"][:10], 1):
                    print(f"\n--- Card {i} ---")
                    print(json.dumps(card, ensure_ascii=False, indent=2))
            else:
                print("æ³¨æ„: bangdan_data ä¸­æ²¡æœ‰ 'cards' å­—æ®µæˆ– cards ä¸ºç©º")
                if "cards" in bangdan_data:
                    print(f"cards ç±»å‹: {type(bangdan_data['cards'])}")
                print()

            # åªå¤„ç†ç¬¬ä¸€ä¸ªæœ‰æ•ˆæ•°æ®ï¼Œç„¶ååœæ­¢
            print("\n" + "=" * 80)
            print("æ¢ç´¢å®Œæˆï¼Œç¨‹åºåœæ­¢")
            print("=" * 80)
            return

    print("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„bangdanæ•°æ®")


def build_entertainment_vocab(
    year: int, top_n: int = 5000, output_file: str = None, mode: str = "test"
):
    """
    æ„å»ºå¨±ä¹è¯æ±‡è¡¨ï¼šä»bangdanæ•°æ®ä¸­æå–åè¯å¹¶æŒ‰é¢‘ç‡æ’åº

    Args:
        year: å¹´ä»½
        top_n: è¾“å‡ºå‰Nä¸ªé«˜é¢‘è¯ï¼Œé»˜è®¤5000
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™è‡ªåŠ¨ç”Ÿæˆ
    """
    if not JIEBA_AVAILABLE:
        print("âŒ jiebaæœªå®‰è£…ï¼Œæ— æ³•æ‰§è¡Œã€‚è¯·è¿è¡Œ: pip install jieba")
        return

    print(f"\n{'='*70}")
    print(f"å¼€å§‹æ„å»º {year} å¹´å¨±ä¹è¯æ±‡è¡¨ï¼ˆåè¯æå–ï¼‰")
    print(f"{'='*70}\n")

    # è®¾ç½®è¾“å‡ºæ–‡ä»¶
    if output_file is None:
        output_file = f"wordlists/entertainment_nouns_{year}.txt"

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(output_file), exist_ok=True)

    if mode != "test":
        unzip_all_bangdan_files(year)

    data_dir = get_bangdan_unzipped_files_dir(year)

    # è·å–æ‰€æœ‰bangdanæ–‡ä»¶
    bangdan_files = [
        os.path.join(data_dir, f)
        for f in os.listdir(data_dir)
        if f.startswith("weibo_bangdan.")
    ]

    if not bangdan_files:
        print(f"âŒ æœªæ‰¾åˆ°bangdanæ–‡ä»¶åœ¨: {data_dir}")
        return

    bangdan_files.sort()
    print(f"âœ“ æ‰¾åˆ° {len(bangdan_files)} ä¸ªbangdanæ–‡ä»¶\n")

    # æå–æ‰€æœ‰åè¯
    all_nouns = defaultdict(int)
    print("å¼€å§‹å¤„ç†æ–‡ä»¶...")

    for i, file_path in enumerate(bangdan_files, 1):
        if i % 30 == 0 or i == 1:  # æ¯30ä¸ªæ–‡ä»¶æ‰“å°ä¸€æ¬¡è¿›åº¦
            print(f"  è¿›åº¦: {i}/{len(bangdan_files)} ({i/len(bangdan_files)*100:.1f}%)")

        nouns = extract_hotwords_from_bangdan_file(file_path, verbose=False)
        for noun in nouns:
            all_nouns[noun] += 1

    print(f"\nâœ“ å¤„ç†å®Œæˆï¼å…±æå– {len(all_nouns)} ä¸ªåè¯ï¼ˆå«é‡å¤ï¼‰\n")

    # æŒ‰é¢‘ç‡æ’åº
    sorted_nouns = sorted(all_nouns.items(), key=lambda x: x[1], reverse=True)
    sorted_nouns = sorted_nouns[:top_n]

    # è¾“å‡ºåˆ°æ–‡ä»¶
    with open(output_file, "w", encoding="utf-8") as f:
        for noun, count in sorted_nouns:
            f.write(f"{noun}\n")

    print(f"{'='*70}")
    print(f"âœ… è¯æ±‡è¡¨å·²ä¿å­˜åˆ°: {output_file}")
    print(f"{'='*70}\n")

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("ğŸ“Š é«˜é¢‘åè¯ Top 30:\n")
    print(f"{'æ’å':<6} {'åè¯':<10} {'é¢‘æ¬¡':<10}")
    print("-" * 35)
    for i, (noun, count) in enumerate(sorted_nouns[:30], 1):
        print(f"{i:<6} {noun:<10} {count:<10}")

    print(f"\n{'='*70}")
    print(f"ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  æ€»åè¯æ•°ï¼ˆå«é‡å¤ï¼‰: {len(all_nouns):,}")
    print(f"  å”¯ä¸€åè¯æ•°: {len(noun_counter):,}")
    print(f"  è¾“å‡ºè¯æ±‡æ•°: {min(top_n, len(sorted_nouns)):,}")
    print(f"  æœ€é«˜é¢‘æ¬¡: {sorted_nouns[0][1] if sorted_nouns else 0}")
    print(
        f"  æœ€ä½é¢‘æ¬¡ï¼ˆTop {top_n}ï¼‰: {sorted_nouns[min(top_n-1, len(sorted_nouns)-1)][1] if sorted_nouns else 0}"
    )
    print(f"{'='*70}")
    print(f"\nğŸ’¡ æç¤º: è¯·æ‰‹å·¥å®¡æŸ¥è¾“å‡ºæ–‡ä»¶ï¼Œç­›é€‰å‡ºå¨±ä¹ç›¸å…³çš„åè¯\n")


class BangdanAnalyzer(object):

    def __init__(
        self,
        year: int,
    ):
        self.year = year
        self.data_dir = get_bangdan_unzipped_files_dir(year)
        self.bangdan_type = "1"

    def get_file_path(self, date: str = None):
        # date should be yyyy-mm-dd format
        return os.path.join(self.data_dir, f"weibo_bangdan.{date}")

    def get_bangdan_text_from_file(self, file_path: str, date: str):
        """
        ä¸€è¡Œbangdanä¿¡æ¯çš„æ ¼å¼ï¼štimestamp,date,text,hot,rear
        ä¾‹å¦‚ï¼š1111111111,2022-01-01,è¿™æ˜¯ä¸€ä¸ªçƒ­æœè¯é¢˜,10000000,100
        """

        bangdan_text_list = []

        # è€ƒè™‘file pathæ˜¯å¦å­˜åœ¨
        if not os.path.exists(file_path):
            print(f"File not exists: {file_path}")
            return None
        with open(file_path, "r", errors="replace") as rfile:
            for line in rfile.readlines():
                line = line.strip()
                line_data = line.split("\t")
                if len(line_data) < 2:
                    print("line data cannot be splitted")
                    continue
                try:
                    data = json.loads(line_data[1])
                except json.JSONDecodeError as e:
                    print(f"JSONDecodeError: {e}")
                    # æ‰“å°å‡ºé”™è¯¯ä½ç½®
                    print(f"Error at line {e.lineno}, column {e.colno}")
                    # æ‰“å°å‡ºé”™è¯¯å­—ç¬¦ä½ç½®
                    print(
                        f"Error at character {e.pos}, {line_data[1][int(e.pos)-20: int(e.pos)+20]}"
                    )
                    continue
                crawler_time_stamp = data["crawler_time_stamp"]
                if data["type"] != self.bangdan_type:
                    # print(f"wrong data type: {data['type']}")
                    # æ’é™¤ä¸å…è®¸çš„æ¦œå•ç±»å‹
                    # ä¸æ˜¯å®æ—¶æ¦œ
                    continue
                data = json.loads(data["bangdan"])
                if type(data) is not dict:
                    print(f"bad data type")
                    print(data)
                    continue
                if "cards" not in data.keys() or data["cards"] is None:
                    print(f"bad data type in file {file_path}")
                    continue
                for card in data["cards"]:
                    if str(card["card_type"]) != "11":
                        continue
                    card_group = card["card_group"]
                    for s_card in card_group:
                        if str(s_card["card_type"]) != "4":
                            continue
                        if "desc" in s_card.keys():
                            text = s_card["desc"]
                            if len(text) <= 5:
                                # å¤ªçŸ­çš„è¯é¢˜ä¸¢æ‰
                                continue

                            hot = ""
                            if "desc_extr" in s_card.keys():
                                # è®¨è®ºå°äº10wçš„ä¸¢æ‰
                                # print(s_card["desc_extr"])
                                hot_number = re.findall(
                                    r"\d+", str(s_card["desc_extr"])
                                )
                                hot = hot_number[0] if len(hot_number) > 0 else None

                            is_rear = (
                                1
                                if re.search(self.rear_pattern, text) is not None
                                else 0
                            )

                            bangdan_text_list.append(
                                f"{crawler_time_stamp},{date},{text},{hot},{is_rear}"
                            )

                        else:
                            print(
                                f"desc not in keys! file_name {file_path}, data: {s_card}"
                            )
        return bangdan_text_list

    def analyze(self):
        # éå†self.yearçš„ä¸€æ•´å¹´çš„æ¯ä¸€å¤© (é€šè¿‡datetime)
        for date in [datetime(self.year, 1, 1) + timedelta(days=i) for i in range(365)]:
            date_str = date.strftime("%Y-%m-%d")
            month_str = date.strftime("%Y-%m")
            file_path = self.get_file_path(date_str)
            if not os.path.exists(file_path):
                print(f"File not exists: {file_path}")
                continue
            bangdan_text_list = self.get_bangdan_text_from_file(file_path, date_str)
            if bangdan_text_list is None:
                continue
            with open(f"bangdan_working_data/{month_str}.csv", "a") as wfile:
                wfile.write("\n".join(bangdan_text_list))
                wfile.write("\n")
            print(f"processed {date_str} in year {self.year}")


if __name__ == "__main__":
    fire.Fire(
        {
            "explore": explore_bangdan_data,
            "build": build_entertainment_vocab,
            "analyze": BangdanAnalyzer,  # ä¿ç•™æ—§çš„analyzeåŠŸèƒ½
        }
    )
