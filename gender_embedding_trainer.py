"""
æ€§åˆ«è¯çš„Word2Vecæ¨¡å‹è®­ç»ƒå™¨

åŠŸèƒ½ï¼š
1. æŒ‰çœä»½åˆ†ç»„æ•°æ®è®­ç»ƒWord2Vecæ¨¡å‹
2. ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹ä¾›åç»­åˆ†æä½¿ç”¨

è¾“å…¥æ•°æ®ï¼š
- 2020: cleaned_weibo_cov/{year}/ ä¸‹çš„parquetæ–‡ä»¶
- 2024: weibo_data_2024/ ä¸‹çš„parquetæ–‡ä»¶ï¼ˆéœ€å…ˆprepareï¼‰
è¾“å‡ºï¼šembedding_models/{year}/ ä¸‹çš„æ¨¡å‹æ–‡ä»¶
"""

import os
import pandas as pd
import numpy as np
from gensim.models import Word2Vec
from collections import defaultdict
import jieba_fast as jieba
import fastparquet as fp
from fastparquet import ParquetFile
import fire
import warnings
import glob
import re
from datetime import datetime, timedelta

import gc
from collections import defaultdict


warnings.filterwarnings("ignore")

DATA_DIR_2020 = "cleaned_weibo_cov"
DATA_DIR_2024 = "../cleaned_weibo_data"
PREPARED_DIR_2024 = "gender_embedding/prepared_weibo_2024"
OUTPUT_DIR = "gender_embedding/embedding_models"

os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(PREPARED_DIR_2024, exist_ok=True)

# çœä»½ç¼–ç æ˜ å°„ï¼ˆGB/T 2260 ä¸­åäººæ°‘å…±å’Œå›½è¡Œæ”¿åŒºåˆ’ä»£ç ï¼‰
PROVINCE_CODE_TO_NAME = {
    "11": "åŒ—äº¬",
    "12": "å¤©æ´¥",
    "13": "æ²³åŒ—",
    "14": "å±±è¥¿",
    "15": "å†…è’™å¤",
    "21": "è¾½å®",
    "22": "å‰æ—",
    "23": "é»‘é¾™æ±Ÿ",
    "31": "ä¸Šæµ·",
    "32": "æ±Ÿè‹",
    "33": "æµ™æ±Ÿ",
    "34": "å®‰å¾½",
    "35": "ç¦å»º",
    "36": "æ±Ÿè¥¿",
    "37": "å±±ä¸œ",
    "41": "æ²³å—",
    "42": "æ¹–åŒ—",
    "43": "æ¹–å—",
    "44": "å¹¿ä¸œ",
    "45": "å¹¿è¥¿",
    "46": "æµ·å—",
    "50": "é‡åº†",
    "51": "å››å·",
    "52": "è´µå·",
    "53": "äº‘å—",
    "54": "è¥¿è—",
    "61": "é™•è¥¿",
    "62": "ç”˜è‚ƒ",
    "63": "é’æµ·",
    "64": "å®å¤",
    "65": "æ–°ç–†",
    "71": "ä¸­å›½å°æ¹¾",
    "81": "ä¸­å›½é¦™æ¸¯",
    "82": "ä¸­å›½æ¾³é—¨",
    # "100": "æœªçŸ¥",
    # "400": "æœªçŸ¥",
}

# åå‘æ˜ å°„ï¼šçœä»½åç§°åˆ°ç¼–ç 
PROVINCE_NAME_TO_CODE = {v: k for k, v in PROVINCE_CODE_TO_NAME.items() if v != "æœªçŸ¥"}

# é€šç”¨åœç”¨è¯
STOPWORDS = set(
    [
        "çš„",
        "æ˜¯",
        "äº†",
        "åœ¨",
        "æœ‰",
        "å’Œ",
        "å°±",
        "ä¸",
        "äºº",
        "éƒ½",
        "ä¸€",
        "ä¸€ä¸ª",
        "ä¸Š",
        "ä¹Ÿ",
        "å¾ˆ",
        "åˆ°",
        "è¯´",
        "è¦",
        "å»",
        "ä½ ",
        "ä¼š",
        "ç€",
        "æ²¡æœ‰",
        "çœ‹",
        "å¥½",
        "è‡ªå·±",
        "è¿™",
        "é‚£",
        "æˆ‘",
        "ä»–",
        "å¥¹",
        "æˆ‘ä»¬",
        "ä½ ä»¬",
        "ä»–ä»¬",
        "å¥¹ä»¬",
        "ä»€ä¹ˆ",
        "æ€ä¹ˆ",
        "è¿™ä¸ª",
        "é‚£ä¸ª",
    ]
)


def clean_weibo_content_2024(text):
    if pd.isna(text) or text == "":
        return ""

    text = str(text)

    text = text.split("//")[0]

    text = (
        text.replace("â€œ", "")
        .replace("â€", "")
        .replace("â€¦", "")
        .replace("ç‚¹å‡»é“¾æ¥æŸ¥çœ‹æ›´å¤š->", "")
    )
    text = text.replace("\u200b", "")

    text = re.sub(r"\[.*?\]", "", text)
    text = re.sub(r"http[s]?://\S+", "", text)
    text = re.sub(r"www\.\S+", "", text)
    text = re.sub(r"t\.cn/\S+", "", text)
    text = re.sub(r"@\S+", "", text)

    return text.strip()


def clean_region_name_2024(region):
    if pd.isna(region) or region == "":
        return None

    region = str(region).strip()

    if region.startswith("å‘å¸ƒäº "):
        region = region[4:].strip()

    return region if region else None


def prepare_2024_data_by_month_group(year, start_month, end_month):
    print(f"\n{'='*60}")
    print(f"ğŸ“¦ å‡†å¤‡ {year} å¹´ {start_month}-{end_month} æœˆæ•°æ®")
    print(f"{'='*60}\n")

    start_date = datetime(year, start_month, 1)
    if end_month == 12:
        end_date = datetime(year, 12, 31)
    else:
        end_date = datetime(year, end_month + 1, 1) - timedelta(days=1)

    date_range = [
        start_date + timedelta(days=n) for n in range((end_date - start_date).days + 1)
    ]

    allowed_provinces = list(PROVINCE_CODE_TO_NAME.values())

    for current_date in date_range:
        date_str = current_date.strftime("%Y-%m-%d")
        print(f"\nå¤„ç†æ—¥æœŸ: {date_str}")

        pattern = os.path.join(DATA_DIR_2024, f"{date_str}.parquet.temp_*")
        print(pattern)
        temp_files = sorted(glob.glob(pattern))

        if not temp_files:
            print(f"  âš ï¸  æœªæ‰¾åˆ°æ–‡ä»¶: {date_str}")
            continue

        print(f"  ğŸ“‚ æ‰¾åˆ° {len(temp_files)} ä¸ªä¸´æ—¶æ–‡ä»¶")

        province_data = defaultdict(list)

        for temp_file in temp_files:
            try:
                df = pd.read_parquet(temp_file)

                if "weibo_content" not in df.columns or "region_name" not in df.columns:
                    print(f"  âš ï¸  è·³è¿‡æ–‡ä»¶ {temp_file}ï¼Œç¼ºå°‘å¿…è¦åˆ—")
                    continue

                df = df.dropna(subset=["weibo_id", "weibo_content", "region_name"])

                df["weibo_content"] = df["weibo_content"].apply(
                    clean_weibo_content_2024
                )
                # å¦‚æœweibo_contenté•¿åº¦å°äº5ï¼Œåˆ é™¤
                df = df[df["weibo_content"].apply(len) > 5]
                df["province"] = df["region_name"].apply(clean_region_name_2024)

                df = df.dropna(subset=["province"])

                df = df[df["province"].isin(allowed_provinces)]
                # df = df[df["weibo_content"] != ""]

                df = df[["weibo_id", "weibo_content", "province"]]

                for province in df["province"].unique():
                    province_df = df[df["province"] == province].copy()
                    province_data[province].append(province_df)

                del df

            except Exception as e:
                print(f"  âŒ è¯»å–æ–‡ä»¶ {temp_file} å¤±è´¥: {e}")
                continue

        for province, data_list in province_data.items():
            if not data_list:
                continue

            combined_data = pd.concat(data_list, ignore_index=True)
            combined_data = combined_data.drop_duplicates(subset=["weibo_id"])
            del data_list

            province_dir = os.path.join(PREPARED_DIR_2024, province)
            os.makedirs(province_dir, exist_ok=True)

            output_file = os.path.join(province_dir, f"{date_str}.parquet")
            combined_data.to_parquet(output_file, engine="fastparquet", index=False)

            print(f"  âœ“ {province}: {len(combined_data):,} æ¡ â†’ {output_file}")
            del combined_data

        import gc

        gc.collect()

    print(f"\nâœ… {start_month}-{end_month} æœˆæ•°æ®å‡†å¤‡å®Œæˆ")


def batch_cut(texts):
    return [jieba.lcut(t, HMM=True) for t in texts]


CHINESE_RE = re.compile(r"[\u4e00-\u9fff]+")


def preprocess_batch(ser: pd.Series) -> list[str]:
    ser = ser.dropna()
    ser = ser[ser.astype(str).str.strip().ne("")]
    chinese = ser.str.findall(CHINESE_RE).str.join("")
    cuts = batch_cut(chinese.tolist())
    out = []
    for words in cuts:
        tmp = [w.strip() for w in words if w.strip() not in STOPWORDS]
        if len(tmp) >= 5:
            out.append(" ".join(tmp))
    return out


class RollingFile:
    def __init__(self, base_dir, prefix="corpus", max_bytes=1024 * 1024 * 1024):
        self.base_dir = base_dir
        self.prefix = prefix
        self.max_bytes = max_bytes
        self.index = 0
        self.bytes = 0
        self._open_next()

    def _open_next(self):
        while True:
            name = f"{self.prefix}_{self.index:06d}"
            path = os.path.join(self.base_dir, name)
            if not os.path.exists(path):
                break
            self.index += 1
        self.f = open(path, "w", buffering=8 * 1024 * 1024, encoding="utf-8")
        self.bytes = 0

    def write(self, text: str):
        if self.bytes + len(text) > self.max_bytes:
            self.f.close()
            self.index += 1
            self._open_next()
        self.f.write(text + "\n")
        self.bytes += len(text)

    def close(self):
        self.f.close()


def clean_2024_for_word2vec(provinces):
    # éå†æ‰€æœ‰çœä»½
    for province in provinces:
        province_folder = os.path.join(PREPARED_DIR_2024, province)
        if not os.path.exists(province_folder):
            continue
        parquet_files = sorted(glob.glob(os.path.join(province_folder, "*.parquet")))
        roller = RollingFile(province_folder)
        total = 0

        for pq_path in parquet_files:
            pf = ParquetFile(pq_path)
            df = pf.to_pandas(columns=["weibo_content"])
            lines = preprocess_batch(df["weibo_content"])
            if lines:
                out_text = "\n".join(lines)
                roller.write(out_text)
                total += len(lines)

            os.remove(pq_path)
        roller.close()
        print(f"  âœ“ {province}: {total} æ¡æ•°æ®")


def load_data_by_province_2020(year):
    year_dir = os.path.join(DATA_DIR_2020, str(year))
    if not os.path.exists(year_dir):
        print(f"âŒ æœªæ‰¾åˆ° {year} å¹´çš„æ•°æ®ç›®å½•")
        return None

    pattern = os.path.join(year_dir, "*.parquet")
    parquet_files = sorted(glob.glob(pattern))

    if not parquet_files:
        print(f"âŒ æœªæ‰¾åˆ° {year} å¹´çš„æ•°æ®æ–‡ä»¶")
        return None

    print(f"ğŸ“‚ æ‰¾åˆ° {len(parquet_files)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹åŠ è½½...")

    required_columns = ["weibo_content"]
    province_col = "province"
    required_columns.append("province")

    data_by_province = defaultdict(list)

    for file_idx, file_path in enumerate(parquet_files):
        try:
            df = pd.read_parquet(file_path, columns=required_columns)
            df = df.dropna(subset=[province_col, "weibo_content"])

            def convert_province_code(code):
                if pd.isna(code):
                    return None
                if isinstance(code, (int, float)):
                    code_str = str(int(code))
                else:
                    code_str = str(code).strip()

                if code_str in PROVINCE_CODE_TO_NAME:
                    return PROVINCE_CODE_TO_NAME[code_str]
                elif code_str in PROVINCE_NAME_TO_CODE.values():
                    return code_str
                else:
                    if not hasattr(convert_province_code, "_warned_codes"):
                        convert_province_code._warned_codes = set()
                    if code_str not in convert_province_code._warned_codes:
                        print(f"  âš ï¸  å‘ç°æœªçŸ¥çœä»½ç¼–ç : {code_str}ï¼Œå°†ä¿ç•™åŸå€¼")
                        convert_province_code._warned_codes.add(code_str)
                    return code_str

            df[province_col] = df[province_col].apply(convert_province_code)
            df = df.dropna(subset=[province_col])

            for province in df[province_col].unique():
                province_data = df[df[province_col] == province].copy()
                data_by_province[province].append(province_data)

            del df

            if (file_idx + 1) % 10 == 0:
                print(f"  å·²å¤„ç† {file_idx + 1}/{len(parquet_files)} ä¸ªæ–‡ä»¶...")

        except Exception as e:
            print(f"âŒ è¯»å–æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
            continue

    print(f"\nğŸ“Š æŒ‰çœä»½åˆ†ç»„ï¼Œå…± {len(data_by_province)} ä¸ªçœä»½")

    converted_provinces = sorted(data_by_province.keys())
    print(
        f"  è½¬æ¢åçš„çœä»½åˆ—è¡¨: {', '.join(converted_provinces[:10])}{'...' if len(converted_provinces) > 10 else ''}"
    )

    result = {}
    for province, data_list in data_by_province.items():
        combined_data = pd.concat(data_list, ignore_index=True)
        del data_list

        if len(combined_data) > 1000:
            result[province] = combined_data
            print(f"  âœ“ {province}: {len(combined_data):,} æ¡æ•°æ®")
        else:
            print(f"  âœ— {province}: {len(combined_data):,} æ¡æ•°æ® (è·³è¿‡ï¼Œæ•°æ®é‡ä¸è¶³)")
            del combined_data

    return result


class ProvinceCorpus:
    def __init__(self, province):
        self.province_dir = os.path.join(PREPARED_DIR_2024, province)
        self.province_files = sorted(
            glob.glob(os.path.join(self.province_dir, "corpus_*"))
        )

    def __iter__(self):
        for file in self.province_files:
            with open(file, "r", buffering=8 * 1024 * 1024) as f:
                for line in f:
                    yield line.strip().split(" ")


def train_word2vec(corpus, vector_size=300, window=5, min_count=20, workers=None):
    """
    è®­ç»ƒWord2Vecæ¨¡å‹ï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ï¼‰

    å‚æ•°è°ƒæ•´è¯´æ˜ï¼š
    - vector_size: 300ï¼ˆæ›´å¤§çš„å‘é‡ç»´åº¦ï¼Œæ›´å¥½çš„è¯­ä¹‰è¡¨è¾¾ï¼‰
    - window: 5ï¼ˆä¸Šä¸‹æ–‡çª—å£ï¼‰
    - min_count: 20ï¼ˆè¯é¢‘é˜ˆå€¼ï¼Œæ ¹æ®æ•°æ®é‡è°ƒæ•´ï¼‰
    - workers: çº¿ç¨‹æ•°ï¼ŒNoneåˆ™è‡ªåŠ¨è®¾ç½®ä¸ºCPUæ ¸å¿ƒæ•°-1
    """
    # è‡ªåŠ¨è®¾ç½®workersï¼Œé¿å…è¶…è¿‡CPUæ ¸å¿ƒæ•°
    if workers is None:
        import multiprocessing

        workers = max(1, multiprocessing.cpu_count() - 1)

    # é™åˆ¶workersæ•°é‡ï¼Œé¿å…å†…å­˜è¿‡åº¦å ç”¨
    workers = min(workers, 8)

    model = Word2Vec(
        sentences=corpus,
        vector_size=vector_size,
        window=window,
        min_count=min_count,
        workers=workers,
        epochs=10,
        sg=1,  # Skip-gram
        negative=10,  # è´Ÿé‡‡æ ·
        seed=42,  # å¯é‡å¤æ€§
        max_vocab_size=None,
    )

    model.wv.fill_norms()

    return model


def train_single_province(province, year, force_retrain=False):
    """è®­ç»ƒå•ä¸ªçœä»½çš„Word2Vecæ¨¡å‹

    Args:
        province: çœä»½åç§°
        year: å¹´ä»½
        force_retrain: å¦‚æœä¸ºTrueï¼Œå³ä½¿æ¨¡å‹æ–‡ä»¶å·²å­˜åœ¨ä¹Ÿé‡æ–°è®­ç»ƒ
    """
    print(f"\n{'='*60}")
    print(f"ğŸ”§ è®­ç»ƒçœä»½: {province}")
    print(f"{'='*60}")

    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
    year_output_dir = os.path.join(OUTPUT_DIR, str(year))
    os.makedirs(year_output_dir, exist_ok=True)
    model_path = os.path.join(year_output_dir, f"model_{province}.model")

    if not force_retrain and os.path.exists(model_path):
        print(f"  â­ï¸  æ¨¡å‹æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡è®­ç»ƒ: {model_path}")
        print(f"  ğŸ’¡ å¦‚éœ€é‡æ–°è®­ç»ƒï¼Œè¯·è®¾ç½® force_retrain=True")
        # å°è¯•åŠ è½½æ¨¡å‹ä»¥è·å–è¯æ±‡è¡¨å¤§å°
        try:
            from gensim.models import KeyedVectors

            wv = KeyedVectors.load(model_path)
            vocab_size = len(wv)
            print(f"  âœ“ å·²å­˜åœ¨æ¨¡å‹ï¼Œè¯æ±‡è¡¨å¤§å°: {vocab_size:,}")
            return {
                "province": province,
                "vocab_size": vocab_size,
            }
        except Exception as e:
            print(f"  âš ï¸  æ— æ³•åŠ è½½ç°æœ‰æ¨¡å‹: {e}ï¼Œå°†é‡æ–°è®­ç»ƒ")

    corpus = ProvinceCorpus(province)
    model = train_word2vec(corpus)
    if model is None:
        print(f"  âŒ è®­ç»ƒæ¨¡å‹å¤±è´¥")
        return None
    vocab_size = len(model.wv)
    print(f"  âœ“ æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œè¯æ±‡è¡¨å¤§å°: {vocab_size:,}")

    stats = {
        "province": province,
        "vocab_size": vocab_size,
    }

    model.wv.save(model_path)
    print(f"  ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")

    del model
    gc.collect()

    return stats


def train_models_by_province_2020(
    data_by_province, year, province_filter=None, force_retrain=False
):
    training_stats = []

    for province, data in data_by_province.items():
        if province_filter and province != province_filter:
            continue

        stats = train_single_province(province, year, force_retrain)
        if stats:
            training_stats.append(stats)

    return training_stats


def train_models_by_province_2024(provinces, year, force_retrain=False):
    training_stats = []

    for province in provinces:
        print(f"\nğŸ“‚ å¤„ç†çœä»½: {province}")
        stats = train_single_province(province, year, force_retrain)
        if stats:
            training_stats.append(stats)

    return training_stats


def save_training_stats(training_stats, year):
    """ä¿å­˜è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
    if not training_stats:
        print("âŒ æ²¡æœ‰è®­ç»ƒç»Ÿè®¡ä¿¡æ¯")
        return

    stats_df = pd.DataFrame(training_stats)
    stats_file = os.path.join(OUTPUT_DIR, f"training_stats_{year}.csv")
    stats_df.to_csv(stats_file, index=False, encoding="utf-8-sig")
    print(f"\nâœ“ è®­ç»ƒç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜: {stats_file}")


PROVINCE_GROUPS = [
    ["åŒ—äº¬", "å¤©æ´¥", "æ²³åŒ—", "å±±è¥¿"],
    ["å†…è’™å¤", "è¾½å®", "å‰æ—", "é»‘é¾™æ±Ÿ"],
    ["ä¸Šæµ·", "æ±Ÿè‹", "æµ™æ±Ÿ", "å®‰å¾½"],
    ["ç¦å»º", "æ±Ÿè¥¿", "å±±ä¸œ"],
    ["æ²³å—", "æ¹–åŒ—", "æ¹–å—"],
    ["å¹¿ä¸œ", "å¹¿è¥¿", "æµ·å—"],
    ["é‡åº†", "å››å·", "è´µå·", "äº‘å—"],
    ["è¥¿è—", "é™•è¥¿", "ç”˜è‚ƒ", "é’æµ·", "å®å¤", "æ–°ç–†"],
    ["ä¸­å›½å°æ¹¾", "ä¸­å›½é¦™æ¸¯", "ä¸­å›½æ¾³é—¨"],
]


def prepare(year: int, start_month: int, end_month: int):
    if year != 2024:
        print("âŒ prepareå‘½ä»¤ä»…æ”¯æŒ2024å¹´æ•°æ®")
        return

    prepare_2024_data_by_month_group(year, start_month, end_month)


def clean(group: int):
    clean_2024_for_word2vec(PROVINCE_GROUPS[group])


def train(year: int, province: str = None, group: int = None):
    print(f"\n{'='*60}")
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ {year} å¹´æ•°æ®çš„Word2Vecæ¨¡å‹")
    print(f"{'='*60}\n")

    if year == 2020:
        data_by_province = load_data_by_province_2020(year)
        if not data_by_province:
            print("âŒ æ— æ³•åŠ è½½æ•°æ®")
            return

        if province:
            if province not in data_by_province:
                print(f"âŒ æœªæ‰¾åˆ°çœä»½: {province}")
                print(f"å¯ç”¨çœä»½: {', '.join(data_by_province.keys())}")
                return
            print(f"ğŸ¯ åªè®­ç»ƒçœä»½: {province}\n")

        training_stats = train_models_by_province_2020(data_by_province, year, province)

    elif year == 2024:
        if group is not None:
            if group < 0 or group >= len(PROVINCE_GROUPS):
                print(
                    f"âŒ æ— æ•ˆçš„åˆ†ç»„ç¼–å·: {group}ï¼Œæœ‰æ•ˆèŒƒå›´: 0-{len(PROVINCE_GROUPS)-1}"
                )
                return
            provinces = PROVINCE_GROUPS[group]
            print(f"ğŸ¯ è®­ç»ƒåˆ†ç»„ {group}: {', '.join(provinces)}\n")
        elif province:
            provinces = [province]
            print(f"ğŸ¯ åªè®­ç»ƒçœä»½: {province}\n")
        else:
            print("âŒ 2024å¹´æ•°æ®å¿…é¡»æŒ‡å®š --group æˆ– --province")
            return

        training_stats = train_models_by_province_2024(provinces, year)

    else:
        print(f"âŒ ä¸æ”¯æŒçš„å¹´ä»½: {year}ï¼Œç›®å‰æ”¯æŒ2020å’Œ2024")
        return

    save_training_stats(training_stats, year)

    print(f"\n{'='*60}")
    print(f"ğŸ‰ {year} å¹´Word2Vecæ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    print(f"{'='*60}\n")


if __name__ == "__main__":
    fire.Fire({"prepare": prepare, "train": train, "clean": clean})
