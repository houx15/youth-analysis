"""
æ€§åˆ«è¯çš„Word2Vecæ¨¡å‹è®­ç»ƒå™¨

åŠŸèƒ½ï¼š
1. æŒ‰çœä»½åˆ†ç»„æ•°æ®è®­ç»ƒWord2Vecæ¨¡å‹
2. ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹ä¾›åç»­åˆ†æä½¿ç”¨

è¾“å…¥æ•°æ®ï¼š
- 2020: cleaned_weibo_cov/{year}/ ä¸‹çš„parquetæ–‡ä»¶
- 2024: weibo_data_2024/ ä¸‹çš„parquetæ–‡ä»¶ï¼ˆéœ€å…ˆprepareï¼‰
è¾“å‡ºï¼šembedding_models/{year}/ ä¸‹çš„æ¨¡å‹æ–‡ä»¶
"""

import os
import pandas as pd
import numpy as np
from gensim.models import Word2Vec
from collections import defaultdict
import jieba
import fire
import warnings
import glob
import re
from datetime import datetime, timedelta

warnings.filterwarnings("ignore")

DATA_DIR_2020 = "cleaned_weibo_cov"
DATA_DIR_2024 = "../cleaned_weibo_data"
PREPARED_DIR_2024 = "gender_embedding/prepared_weibo_2024"
OUTPUT_DIR = "gender_embedding/embedding_models"

os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(PREPARED_DIR_2024, exist_ok=True)

# çœä»½ç¼–ç æ˜ å°„ï¼ˆGB/T 2260 ä¸­åäººæ°‘å…±å’Œå›½è¡Œæ”¿åŒºåˆ’ä»£ç ï¼‰
PROVINCE_CODE_TO_NAME = {
    "11": "åŒ—äº¬",
    "12": "å¤©æ´¥",
    "13": "æ²³åŒ—",
    "14": "å±±è¥¿",
    "15": "å†…è’™å¤",
    "21": "è¾½å®",
    "22": "å‰æ—",
    "23": "é»‘é¾™æ±Ÿ",
    "31": "ä¸Šæµ·",
    "32": "æ±Ÿè‹",
    "33": "æµ™æ±Ÿ",
    "34": "å®‰å¾½",
    "35": "ç¦å»º",
    "36": "æ±Ÿè¥¿",
    "37": "å±±ä¸œ",
    "41": "æ²³å—",
    "42": "æ¹–åŒ—",
    "43": "æ¹–å—",
    "44": "å¹¿ä¸œ",
    "45": "å¹¿è¥¿",
    "46": "æµ·å—",
    "50": "é‡åº†",
    "51": "å››å·",
    "52": "è´µå·",
    "53": "äº‘å—",
    "54": "è¥¿è—",
    "61": "é™•è¥¿",
    "62": "ç”˜è‚ƒ",
    "63": "é’æµ·",
    "64": "å®å¤",
    "65": "æ–°ç–†",
    "71": "å°æ¹¾",
    "81": "é¦™æ¸¯",
    "82": "æ¾³é—¨",
    "100": "æœªçŸ¥",
    "400": "æœªçŸ¥",
}

# åå‘æ˜ å°„ï¼šçœä»½åç§°åˆ°ç¼–ç 
PROVINCE_NAME_TO_CODE = {v: k for k, v in PROVINCE_CODE_TO_NAME.items() if v != "æœªçŸ¥"}

# é€šç”¨åœç”¨è¯
STOPWORDS = set(
    [
        "çš„",
        "æ˜¯",
        "äº†",
        "åœ¨",
        "æœ‰",
        "å’Œ",
        "å°±",
        "ä¸",
        "äºº",
        "éƒ½",
        "ä¸€",
        "ä¸€ä¸ª",
        "ä¸Š",
        "ä¹Ÿ",
        "å¾ˆ",
        "åˆ°",
        "è¯´",
        "è¦",
        "å»",
        "ä½ ",
        "ä¼š",
        "ç€",
        "æ²¡æœ‰",
        "çœ‹",
        "å¥½",
        "è‡ªå·±",
        "è¿™",
        "é‚£",
        "æˆ‘",
        "ä»–",
        "å¥¹",
        "æˆ‘ä»¬",
        "ä½ ä»¬",
        "ä»–ä»¬",
        "å¥¹ä»¬",
        "ä»€ä¹ˆ",
        "æ€ä¹ˆ",
        "è¿™ä¸ª",
        "é‚£ä¸ª",
    ]
)


def clean_weibo_content_2024(text):
    if pd.isna(text) or text == "":
        return ""

    text = str(text)

    text = re.sub(r"\[.*?\]", "", text)
    text = re.sub(r"http[s]?://\S+", "", text)
    text = re.sub(r"www\.\S+", "", text)
    text = re.sub(r"t\.cn/\S+", "", text)
    text = re.sub(r"@\S+", "", text)

    emoji_pattern = re.compile(
        "["
        "\U0001f600-\U0001f64f"
        "\U0001f300-\U0001f5ff"
        "\U0001f680-\U0001f6ff"
        "\U0001f1e0-\U0001f1ff"
        "\U00002702-\U000027b0"
        "\U000024c2-\U0001f251"
        "]+",
        flags=re.UNICODE,
    )
    text = emoji_pattern.sub("", text)

    return text.strip()


def clean_region_name_2024(region):
    if pd.isna(region) or region == "":
        return None

    region = str(region).strip()

    if region.startswith("å‘å¸ƒäº "):
        region = region[4:].strip()

    return region if region else None


def preprocess_text(text):
    if pd.isna(text) or text == "":
        return []

    text = str(text)
    words = jieba.cut(text)
    words = [
        w.strip()
        for w in words
        if w.strip() and w not in STOPWORDS and len(w.strip()) > 1
    ]
    return words


def prepare_2024_data_by_month_group(year, start_month, end_month):
    print(f"\n{'='*60}")
    print(f"ğŸ“¦ å‡†å¤‡ {year} å¹´ {start_month}-{end_month} æœˆæ•°æ®")
    print(f"{'='*60}\n")

    start_date = datetime(year, start_month, 1)
    if end_month == 12:
        end_date = datetime(year, 12, 31)
    else:
        end_date = datetime(year, end_month + 1, 1) - timedelta(days=1)

    date_range = [
        start_date + timedelta(days=n) for n in range((end_date - start_date).days + 1)
    ]

    for current_date in date_range:
        date_str = current_date.strftime("%Y-%m-%d")
        print(f"\nå¤„ç†æ—¥æœŸ: {date_str}")

        pattern = os.path.join(DATA_DIR_2024, f"{date_str}.parquet.temp_*")
        print(pattern)
        temp_files = sorted(glob.glob(pattern))

        if not temp_files:
            print(f"  âš ï¸  æœªæ‰¾åˆ°æ–‡ä»¶: {date_str}")
            continue

        print(f"  ğŸ“‚ æ‰¾åˆ° {len(temp_files)} ä¸ªä¸´æ—¶æ–‡ä»¶")

        province_data = defaultdict(list)

        for temp_file in temp_files:
            try:
                df = pd.read_parquet(temp_file)

                if "weibo_content" not in df.columns or "region_name" not in df.columns:
                    print(f"  âš ï¸  è·³è¿‡æ–‡ä»¶ {temp_file}ï¼Œç¼ºå°‘å¿…è¦åˆ—")
                    continue

                df = df.dropna(subset=["weibo_id", "weibo_content", "region_name"])

                df["weibo_content"] = df["weibo_content"].apply(
                    clean_weibo_content_2024
                )
                # å¦‚æœweibo_contenté•¿åº¦å°äº5ï¼Œåˆ é™¤
                df = df[df["weibo_content"].apply(len) > 5]
                df["province"] = df["region_name"].apply(clean_region_name_2024)

                df = df.dropna(subset=["province"])
                # df = df[df["weibo_content"] != ""]

                df = df[["weibo_id", "weibo_content", "province"]]

                for province in df["province"].unique():
                    province_df = df[df["province"] == province].copy()
                    province_data[province].append(province_df)

                del df

            except Exception as e:
                print(f"  âŒ è¯»å–æ–‡ä»¶ {temp_file} å¤±è´¥: {e}")
                continue

        for province, data_list in province_data.items():
            if not data_list:
                continue

            combined_data = pd.concat(data_list, ignore_index=True)
            combined_data = combined_data.drop_duplicates(subset=["weibo_id"])
            del data_list

            province_dir = os.path.join(PREPARED_DIR_2024, province)
            os.makedirs(province_dir, exist_ok=True)

            output_file = os.path.join(province_dir, f"{date_str}.parquet")
            combined_data.to_parquet(output_file, engine="fastparquet", index=False)

            print(f"  âœ“ {province}: {len(combined_data):,} æ¡ â†’ {output_file}")
            del combined_data

        import gc

        gc.collect()

    print(f"\nâœ… {start_month}-{end_month} æœˆæ•°æ®å‡†å¤‡å®Œæˆ")


def load_single_province_2024(province):
    province_dir = os.path.join(PREPARED_DIR_2024, province)

    if not os.path.exists(province_dir):
        return None

    pattern = os.path.join(province_dir, "*.parquet")
    parquet_files = sorted(glob.glob(pattern))

    if not parquet_files:
        return None

    data_list = []
    for file_path in parquet_files:
        try:
            df = pd.read_parquet(file_path)
            data_list.append(df)
        except Exception as e:
            print(f"  âŒ è¯»å–å¤±è´¥: {file_path} - {e}")
            continue

    if not data_list:
        return None

    combined_data = pd.concat(data_list, ignore_index=True)
    del data_list

    combined_data = combined_data.drop_duplicates(subset=["weibo_id"])

    if len(combined_data) < 100000:
        print(f"  âœ— {province}: {len(combined_data):,} æ¡ (æ•°æ®é‡ä¸è¶³)")
        del combined_data
        return None

    return combined_data


def load_data_by_province_2020(year):
    year_dir = os.path.join(DATA_DIR_2020, str(year))
    if not os.path.exists(year_dir):
        print(f"âŒ æœªæ‰¾åˆ° {year} å¹´çš„æ•°æ®ç›®å½•")
        return None

    pattern = os.path.join(year_dir, "*.parquet")
    parquet_files = sorted(glob.glob(pattern))

    if not parquet_files:
        print(f"âŒ æœªæ‰¾åˆ° {year} å¹´çš„æ•°æ®æ–‡ä»¶")
        return None

    print(f"ğŸ“‚ æ‰¾åˆ° {len(parquet_files)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹åŠ è½½...")

    required_columns = ["weibo_content"]
    province_col = "province"
    required_columns.append("province")

    data_by_province = defaultdict(list)

    for file_idx, file_path in enumerate(parquet_files):
        try:
            df = pd.read_parquet(file_path, columns=required_columns)
            df = df.dropna(subset=[province_col, "weibo_content"])

            def convert_province_code(code):
                if pd.isna(code):
                    return None
                if isinstance(code, (int, float)):
                    code_str = str(int(code))
                else:
                    code_str = str(code).strip()

                if code_str in PROVINCE_CODE_TO_NAME:
                    return PROVINCE_CODE_TO_NAME[code_str]
                elif code_str in PROVINCE_NAME_TO_CODE.values():
                    return code_str
                else:
                    if not hasattr(convert_province_code, "_warned_codes"):
                        convert_province_code._warned_codes = set()
                    if code_str not in convert_province_code._warned_codes:
                        print(f"  âš ï¸  å‘ç°æœªçŸ¥çœä»½ç¼–ç : {code_str}ï¼Œå°†ä¿ç•™åŸå€¼")
                        convert_province_code._warned_codes.add(code_str)
                    return code_str

            df[province_col] = df[province_col].apply(convert_province_code)
            df = df.dropna(subset=[province_col])

            for province in df[province_col].unique():
                province_data = df[df[province_col] == province].copy()
                data_by_province[province].append(province_data)

            del df

            if (file_idx + 1) % 10 == 0:
                print(f"  å·²å¤„ç† {file_idx + 1}/{len(parquet_files)} ä¸ªæ–‡ä»¶...")

        except Exception as e:
            print(f"âŒ è¯»å–æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
            continue

    print(f"\nğŸ“Š æŒ‰çœä»½åˆ†ç»„ï¼Œå…± {len(data_by_province)} ä¸ªçœä»½")

    converted_provinces = sorted(data_by_province.keys())
    print(
        f"  è½¬æ¢åçš„çœä»½åˆ—è¡¨: {', '.join(converted_provinces[:10])}{'...' if len(converted_provinces) > 10 else ''}"
    )

    result = {}
    for province, data_list in data_by_province.items():
        combined_data = pd.concat(data_list, ignore_index=True)
        del data_list

        if len(combined_data) > 1000:
            result[province] = combined_data
            print(f"  âœ“ {province}: {len(combined_data):,} æ¡æ•°æ®")
        else:
            print(f"  âœ— {province}: {len(combined_data):,} æ¡æ•°æ® (è·³è¿‡ï¼Œæ•°æ®é‡ä¸è¶³)")
            del combined_data

    return result


def train_word2vec(texts, vector_size=300, window=5, min_count=20, workers=None):
    """
    è®­ç»ƒWord2Vecæ¨¡å‹ï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ï¼‰

    å‚æ•°è°ƒæ•´è¯´æ˜ï¼š
    - vector_size: 300ï¼ˆæ›´å¤§çš„å‘é‡ç»´åº¦ï¼Œæ›´å¥½çš„è¯­ä¹‰è¡¨è¾¾ï¼‰
    - window: 5ï¼ˆä¸Šä¸‹æ–‡çª—å£ï¼‰
    - min_count: 20ï¼ˆè¯é¢‘é˜ˆå€¼ï¼Œæ ¹æ®æ•°æ®é‡è°ƒæ•´ï¼‰
    - workers: çº¿ç¨‹æ•°ï¼ŒNoneåˆ™è‡ªåŠ¨è®¾ç½®ä¸ºCPUæ ¸å¿ƒæ•°-1
    """
    if not texts or len(texts) < 100:
        return None

    # è‡ªåŠ¨è®¾ç½®workersï¼Œé¿å…è¶…è¿‡CPUæ ¸å¿ƒæ•°
    if workers is None:
        import multiprocessing

        workers = max(1, multiprocessing.cpu_count() - 1)

    # é™åˆ¶workersæ•°é‡ï¼Œé¿å…å†…å­˜è¿‡åº¦å ç”¨
    workers = min(workers, 8)

    model = Word2Vec(
        sentences=texts,
        vector_size=vector_size,
        window=window,
        min_count=min_count,
        workers=workers,
        epochs=10,
        sg=1,  # Skip-gram
        negative=10,  # è´Ÿé‡‡æ ·
        seed=42,  # å¯é‡å¤æ€§
        max_vocab_size=None,
    )

    return model


def train_single_province(province, data, year):
    print(f"\n{'='*60}")
    print(f"ğŸ”§ è®­ç»ƒçœä»½: {province}")
    print(f"{'='*60}")

    data_count = len(data)
    print(f"  ğŸ“Š åŸå§‹æ•°æ®: {data_count:,} æ¡")

    texts = []
    for row in data.itertuples():
        words = preprocess_text(row.weibo_content)
        if len(words) > 3:
            texts.append(words)

    del data
    import gc

    gc.collect()

    if len(texts) < 100000:
        print(f"  âŒ æ–‡æœ¬é‡ä¸è¶³ ({len(texts)} æ¡)ï¼Œè·³è¿‡")
        del texts
        return None

    print(f"  ğŸ“ æœ‰æ•ˆæ–‡æœ¬: {len(texts):,} æ¡")

    print(f"  ğŸ”§ è®­ç»ƒWord2Vecæ¨¡å‹...")
    model = train_word2vec(texts)
    if model is None:
        print(f"  âŒ è®­ç»ƒæ¨¡å‹å¤±è´¥")
        del texts
        return None

    vocab_size = len(model.wv)
    print(f"  âœ“ æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œè¯æ±‡è¡¨å¤§å°: {vocab_size:,}")

    stats = {
        "province": province,
        "data_count": data_count,
        "text_count": len(texts),
        "vocab_size": vocab_size,
    }

    del texts
    gc.collect()

    year_output_dir = os.path.join(OUTPUT_DIR, str(year))
    os.makedirs(year_output_dir, exist_ok=True)

    model_path = os.path.join(year_output_dir, f"model_{province}.model")
    model.save(model_path)
    print(f"  ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")

    del model
    gc.collect()

    return stats


def train_models_by_province_2020(data_by_province, year, province_filter=None):
    training_stats = []

    for province, data in data_by_province.items():
        if province_filter and province != province_filter:
            continue

        stats = train_single_province(province, data, year)
        if stats:
            training_stats.append(stats)

    return training_stats


def train_models_by_province_2024(provinces, year):
    training_stats = []

    for province in provinces:
        print(f"\nğŸ“‚ åŠ è½½çœä»½: {province}")
        data = load_single_province_2024(province)

        if data is None:
            print(f"  âš ï¸  è·³è¿‡ {province}ï¼Œæ— æ³•åŠ è½½æ•°æ®")
            continue

        print(f"  âœ“ åŠ è½½æˆåŠŸ: {len(data):,} æ¡æ•°æ®")

        stats = train_single_province(province, data, year)
        if stats:
            training_stats.append(stats)

    return training_stats


def save_training_stats(training_stats, year):
    """ä¿å­˜è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
    if not training_stats:
        print("âŒ æ²¡æœ‰è®­ç»ƒç»Ÿè®¡ä¿¡æ¯")
        return

    stats_df = pd.DataFrame(training_stats)
    stats_file = os.path.join(OUTPUT_DIR, f"training_stats_{year}.csv")
    stats_df.to_csv(stats_file, index=False, encoding="utf-8-sig")
    print(f"\nâœ“ è®­ç»ƒç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜: {stats_file}")


PROVINCE_GROUPS = [
    ["åŒ—äº¬", "å¤©æ´¥", "æ²³åŒ—", "å±±è¥¿"],
    ["å†…è’™å¤", "è¾½å®", "å‰æ—", "é»‘é¾™æ±Ÿ"],
    ["ä¸Šæµ·", "æ±Ÿè‹", "æµ™æ±Ÿ", "å®‰å¾½"],
    ["ç¦å»º", "æ±Ÿè¥¿", "å±±ä¸œ"],
    ["æ²³å—", "æ¹–åŒ—", "æ¹–å—"],
    ["å¹¿ä¸œ", "å¹¿è¥¿", "æµ·å—"],
    ["é‡åº†", "å››å·", "è´µå·", "äº‘å—"],
    ["è¥¿è—", "é™•è¥¿", "ç”˜è‚ƒ", "é’æµ·", "å®å¤", "æ–°ç–†"],
]


def prepare(year: int, start_month: int, end_month: int):
    if year != 2024:
        print("âŒ prepareå‘½ä»¤ä»…æ”¯æŒ2024å¹´æ•°æ®")
        return

    prepare_2024_data_by_month_group(year, start_month, end_month)


def train(year: int, province: str = None, group: int = None):
    print(f"\n{'='*60}")
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ {year} å¹´æ•°æ®çš„Word2Vecæ¨¡å‹")
    print(f"{'='*60}\n")

    if year == 2020:
        data_by_province = load_data_by_province_2020(year)
        if not data_by_province:
            print("âŒ æ— æ³•åŠ è½½æ•°æ®")
            return

        if province:
            if province not in data_by_province:
                print(f"âŒ æœªæ‰¾åˆ°çœä»½: {province}")
                print(f"å¯ç”¨çœä»½: {', '.join(data_by_province.keys())}")
                return
            print(f"ğŸ¯ åªè®­ç»ƒçœä»½: {province}\n")

        training_stats = train_models_by_province_2020(data_by_province, year, province)

    elif year == 2024:
        if group is not None:
            if group < 0 or group >= len(PROVINCE_GROUPS):
                print(
                    f"âŒ æ— æ•ˆçš„åˆ†ç»„ç¼–å·: {group}ï¼Œæœ‰æ•ˆèŒƒå›´: 0-{len(PROVINCE_GROUPS)-1}"
                )
                return
            provinces = PROVINCE_GROUPS[group]
            print(f"ğŸ¯ è®­ç»ƒåˆ†ç»„ {group}: {', '.join(provinces)}\n")
        elif province:
            provinces = [province]
            print(f"ğŸ¯ åªè®­ç»ƒçœä»½: {province}\n")
        else:
            print("âŒ 2024å¹´æ•°æ®å¿…é¡»æŒ‡å®š --group æˆ– --province")
            return

        training_stats = train_models_by_province_2024(provinces, year)

    else:
        print(f"âŒ ä¸æ”¯æŒçš„å¹´ä»½: {year}ï¼Œç›®å‰æ”¯æŒ2020å’Œ2024")
        return

    save_training_stats(training_stats, year)

    print(f"\n{'='*60}")
    print(f"ğŸ‰ {year} å¹´Word2Vecæ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    print(f"{'='*60}\n")


if __name__ == "__main__":
    fire.Fire({"prepare": prepare, "train": train})
